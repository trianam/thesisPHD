\chapter{Machine Learning}
\section{Machine Learning Theory}
\ac{ml} is a branch of \ac{ai}
where the focus is to develop systems that learn from data. \ac{ml} is
a field that is located in the intersection 
of different disciplines:
\begin{description}
\item[statistics] deals with the uncertainty of the world and, as we
  will see, ML can be framed as a probability estimation;
\item[data science] is the science that interprets and studies the
  information contained in the data and how it can be used;
\item[optimization theory] is the mathematical branch that defines
  methods to optimize functions. \ac{ml} can be seen as an optimization
  problem;
\item[computer science] is the science that studies algorithms and
  their complexity;
\item[computer engineering] focuses on the development of efficient
  software.
\end{description}

In order to learn we must provide some data. The data can be of
different kinds. Based
on the type of data and on the problem that we need to resolve, we can denote
three different types of \ac{ml}: supervised, 
unsupervised, and reinforcement learning.

\subsection{Supervised Learning}\label{sec:supervisedTheory}
In supervised learning we have a dataset $S$ of samples $\vect{x}_i$
each one labeled with $\vect{y}_i$:
\begin{equation*}
  S=\left((\vect{x}_1,\vect{y}_1),\dots,(\vect{x}_n,\vect{y}_n)\right),
\end{equation*}
where $\vect{x}_i\in\XSet$ and $\vect{y}_i\in\YSet$. Usually, $\XSet$
is a space of integer or real 
tensors of certain dimensionality, and $\YSet$ is a space of integer
vectors for classification tasks and real vectors for regression
tasks. In binary classification tasks $\YSet={0,1}$, in multilabel
classification $\YSet={0,1}^k$ with $k$ possible labels, and in
multiclass classification $\YSet={0,1}^k$ with only one value equal to
$1$ and the rest equal to $0$. The samples $\vect{x}_i$ of a dataset must be
drawn from the same unknown distribution $\vect{x}_i\sim\dist{D}$. The
relationship between the samples and the labels is defined by an unknown
labeling function $f:\XSet\rightarrow\YSet$ such that for each sample
$\vect{y}_i=f(\vect{x}_i)$.

A learning algorithm receives a training set $S$ as input, and should
output a predictor $h_S:\XSet\rightarrow\YSet$ that minimizes the
\emph{prediction error}. For a generic predictor $h$, the prediction
error is:
\begin{equation}\label{predictionError}
  L_{\dist{D},f}(h)\define\prob_{\vect{x}\sim\dist{D}}[h(\vect{x})\neq
  f(\vect{x})]\define\dist{D}(\{\vect{x}:h(\vect{x})\neq
  f(\vect{x})\}),
\end{equation}
where, for $A\subset\XSet$, the probability $\dist{D}$ assign a
likelihood $\dist{D}(A)$ of observing a value $\vect{x}\in A$. Given
that both $\dist{D}$ and $f$ are unknown, to find $h_S$ the learning
algorithm minimizes the \emph{empirical prediction error} (or
empirical risk):
\begin{equation}\label{empiricalError}
  L_S(h)\define\frac{|i\in\{1,\dots,n\}:h(\vect{x}_i)\neq\vect{y}_i|}{n}.
\end{equation}
This learning paradigm of finding $h_S$ is called \ac{erm}.

\ac{erm} rule may lead to \emph{overfitting} if not restricted. A predictor
can perform well over the training set $S$ --- having a low empirical
error --- but it can perform badly over the entire distribution
$\dist{D}$ with a high prediction error. A solution is to apply
\ac{erm} over a restricted search space. With $\HSet$ we denote the
\emph{hypothesis class} of the possible predictors
$h\in{HSet}:\XSet\rightarrow\YSet$. For a given class $\HSet$ and a
training sample $S$, the $ERM_\HSet$ learner uses the \ac{erm} rule to
choose a predictor $h_S\in\HSet$ with the lowest possible empirical
error over $S$:
\begin{equation*}
  h_S=ERM_\HSet(S)\in \argmin_{h\in\HSet} L_S(h).
\end{equation*}

We induce an \emph{inductive bias} introducing restrictions over
$\HSet$. Intuitively, choosing a more restricted hypothesis class
better protect the model against overfitting but at the same time may cause
a stronger bias.


\section{\acf{svm}}
\acp{svm} \cite{cortes-support-1995} are supervised learning models
commonly used in classification problems. The basic idea of linear
\acp{svm} is to find a hyperplane in the features space that separates
examples belonging to different classes. At prediction time, the model
classifies new examples based on their position relative to the
hyperplane. For a given set of data, infinite separating hyperplanes
could exist. \acp{svm} search for the maximum-margin separating
hyperplane, i.e.\ the one with maximum distance from the nearest
examples.

The basic \ac{svm} algorithm works only if the features belonging to
the two classes are linearly separable. The \emph{soft margin} method
manages the cases where the classes are not linearly separable. A set
of \emph{slack} variables allows for a certain degree of
misclassification. They allow for samples to be on the wrong side of
the split.

The soft margin allows to separate
data that is linearly separable except for some examples. Sets that
are hardly not 
linearly separable sets can be approached using the
\emph{kernel trick}. If we call $S$ the space of the features, the
kernel trick works by finding a transformation $\phi:S\rightarrow V$
from the original space to a new space $V$ where the examples are more
easily separable. \acp{svm} can then be used on the new space $V$.

\section{Neural Networks}
In our work, we used mainly a specific kind of \acp{rnn}: \ac{lstm}.
\acp{rnn} are a class of \acp{ann} where the connections are not only
sequential from one layer to the following one, but they form
loops instead. 

\subsection{\acf{ann}}
\begin{figure}
  \centering
  \begin{tikzpicture}[x=1.5cm, y=1.5cm]
    \node [] (input-1) at (0,0.5) {$x_1$};
    \node [] (input-2) at (0,0) {$x_2$};
    \node [] (input-3) at (0,-0.5) {$x_3$};
    \node [] (input-missing) at (0,-0.9) {$\vdots$};
    \node [] (input-n) at (0,-1.4) {$x_n$};

    \node [every neuron] (neuron) at (2,-0.5) {$\Sigma$};
    \node [bias] (bias) at (2,-1.3) {$b$};

    \node [activation] (act) at (3,-0.5) {$f$};
    \node [] (output) at (4,-0.5) {$y$};
    
    \foreach \i in {1,...,3,n}
    \draw [arrow] (input-\i) -- (neuron)
    node [above=-0.05, pos=0.3, sloped] {$w_\i$};

    \draw [arrow] (bias) -- (neuron);

    \draw [arrow] (neuron) -- (act);
    \draw [arrow] (act) -- (output);
  \end{tikzpicture}
  \caption{Artificial neuron.}
  \label{fig:neuron}
\end{figure}
An \ac{ann} is a model that performs elaborations in a way that
mimics the brain functioning. The base unit is the \ac{an}, also called
perceptron, in
\cref{fig:neuron}. It performs the weighted sum of the inputs
$x_1,\dots,x_n$, shifted by a bias $b$, followed by an activation
function $f$. If we add a dummy input $x_0=1$ and rename the bias
$b=w_0$, we can express the computation of the \ac{an} as in
\cref{eq:anComp}:
\begin{equation}\label{eq:anComp}
  y = f(\sum_{i=0}^n w_i x_i).
\end{equation}

The activation function $f$ can be of different types, the most common are:
\begin{itemize}
\item Softmax;
\item ReLU;
\item TanH;
\item Sigmoid;
\item Linear.
\end{itemize}

\begin{figure}
  \centering
  \begin{tikzpicture}[x=1.5cm, y=1.5cm]
    \foreach \m/\l [count=\y] in {1,2,3,missing,4}
    \node [every neuron/.try, neuron \m/.try] (input-\m) at (0,2.5-\y) {};

    \foreach \m [count=\y] in {1,missing,2}
    \node [every neuron/.try, neuron \m/.try ] (hidden-\m) at (2,2-\y*1.25) {};

    \foreach \m [count=\y] in {1,missing,2}
    \node [every neuron/.try, neuron \m/.try ] (output-\m) at (4,1.5-\y) {};

    \foreach \l [count=\i] in {1,2,3,n}
    \node[] (x-\i) at ($(input-\i)-(1,0)$) {$x_\l$};

    \foreach \l [count=\i] in {1,2,3,n}
    \draw [arrow] (x-\i) -- (input-\i);
    %\draw [arrowInverse] (input-\i) -- ++(-1,0)
    %node [above, midway] {$I_\l$};

    %\foreach \l [count=\i] in {1,n}
    %\node [above] at (hidden-\i.north) {$H_\l$};

    \foreach \l [count=\i] in {1,m}
    \node[] (y-\i) at ($(output-\i)+(1,0)$) {$y_\l$};
    
    \foreach \l [count=\i] in {1,n}
    \draw [arrow] (output-\i) -- (y-\i);
    %\draw [arrow] (output-\i) -- ++(1,0)
    %node [above, midway] {$O_\l$};

    \foreach \i in {1,...,4}
    \foreach \j in {1,...,2}
    \draw [arrow] (input-\i) -- (hidden-\j);

    \foreach \i in {1,...,2}
    \foreach \j in {1,...,2}
    \draw [arrow] (hidden-\i) -- (output-\j);

    \foreach \l [count=\x from 0] in {Input, Hidden, Ouput}
    \node [align=center, above] at (\x*2,2) {\l \\ layer};

  \end{tikzpicture}
  \caption{\acs{mlp} with one hidden layer.}
  \label{fig:ann}
\end{figure}
\acp{an} are organized in network structures. The basic layout of
an  \ac{ann} is the \acf{mlp} structured in layers like in
\cref{fig:ann}. Each \ac{an} of each layer is connected to all the
outputs of the previous layer. The first layer is connected to the
inputs of the \ac{ann} and the output of the last layer is also the
output of the network. The execution of the \ac{mlp} is feed forward:
\begin{enumerate}
\item the input values $x_{0,1},\dots x_{0,n}$ are presented to the
  network and they are the input of the first layer;
\item  the computation is 
  carried one layer at a time, where each neuron $i$ of the layer $l$
  calculates the value $y_{l,i}$ of the intermediate output
  $y_{l,1},\dots,y_{l,m}$;
\item the intermediate output $y_{l,1},\dots,y_{l,m}$ of the layer $l$
  becomes the 
  input $x_{l+1,1},\dots,x_{l+1,m}$ of the subsequent layer $l+1$,
  unless $l$ is the last layer - in that case the output of $l$ is the
  output $y_1,\dots,y_m$ of the \ac{mlp}.
\end{enumerate}

The weights of the \acp{an} are initialized to random values and, in order
to have meaningful outputs, the \ac{ann} needs to be trained. In a
supervised learning framework, the dataset is composed of the matrices
$\matr{X}$ ($N\times n$) of the $N$ inputs $x_{i,j}$ and $\matr{Y}$
($N\times m$) of 
the corresponding outputs $y_{i,j}$. The
training process is called \emph{backpropagation} and it is organized in a
sequence of phases. For each phase $p$ there are two steps:
\begin{description}
\item[execution] where an input $x_{p,1},\dots x_{p,n}$ is given to
  the network and an output $\hat{y}_{p,1},\dots,\hat{y}_{p,m}$ is calculated;
\item[weight update] where the error between
  $\hat{y}_{p,1},\dots,\hat{y}_{p,m}$ and the correct output
  $y_{p,1},\dots,y_{p,m}$ is calculated. This error is back propagated
  through all 
  the layers and a correction $\Delta w_i$ is calculated for each weight $w_i$ of
  the network, in
  order to minimize the error surface in the space of the weights.
\end{description}
In detail, to calculate the weights $\vect{w}^{(p+1)}$ for the next
phase $p+1$, it is 
sufficient to determine the gradient of 
the error surface in the current point $\vect{w}^{(t)}$ in order to
apply an optimization method like \ac{sgd}. We discuss \ac{sgd} in
detail below.

\subsection{Loss}
The goal of the lerning algorithm is to learn a function $f$ such that
predictions $\hat{\vect{y}}=f(\vect{x})$ over the training set are accurate respect
to the correct labels $\vect{y}$. The \emph{loss} function measures
the prediction error. Formally, given the true expected output
$\vect{y}$, a loss function
$\loss(\hat{\vect{y}},\vect{y})$ assigns a scalar to a predicted
output $\hat{\vect{y}}$. The loss function should be lower bounded with the
minimum value attained only for cases where the prediction is correct. The
parameters of the learned function, i.e. the weights $\vect{w}$, are
set in order to minimize the loss over the training examples. Given a
labeled training set $\mathcal{D}=(\vect{x}_{1:n},\vect{y}_{1:n})$ and a
parameterized model $f(\vect{x};\theta)$, the goal of the training
algorithm is then to set the values of the parameters $\theta$
such that the value of the loss is minimized:
\begin{equation}\label{eq:lossmin}
  \hat\theta=\argmin_\theta\frac{1}{n}\sum_{i=1}^n\loss(f(\vect{x}_i;\theta),\vect{y}_i).
\end{equation}

The parameters $\theta$ represent the set of all the weights $w$ of
the \ac{ann}. 
We proceed to describe common loss functions.

\subsubsection{Hinge loss}
Hinge loss (also known as margin loss) is used in binary classification
problems, when the classifier output $\tilde{y}$ is a single scalar and
$y\in\{+1,-1\}$. The classification rule is $\hat{y}=sign(\tilde{y})$
and the classification is considered correct if
$y\cdot\tilde{y}>0$. The hinge loss is defined as:
\begin{equation*}
  \loss_{hinge}(\tilde{y},y)=\max(0,1-y\cdot\tilde{y}).
\end{equation*}
It is $0$ when $\tilde{y}$ and $y$ share the same sign and
$|\tilde{y}|\geq 1$, otherwise the loss is linear. It attempts to
achieve a correct classification with a margin of at least 1.

Hinge loss can be also extended to multi-class settings, where
$\hat{\vect{y}}=\hat{y}_1,\dots,\hat{y}_n$ are the classifier's
output and $\vect{y}$ the one-hot vector of correct output
classes. The classification rule is defined as selecting the class
with the highest score $\argmax_i\hat{\vect{y}}_i$. Denoting by
$t=\argmax_iy_i$ the correct class index, and by $k=\argmax_{i\neq
  t}\hat{y}_i$ the highest scoring class such that $k\neq t$, the
multi-class hinge loss is defined as:
\begin{equation*}
  \loss_{hinge}(\hat{\vect{y}},
  \vect{y})=\max(0,1-(\hat{y}_t-\hat{y}_k)).
\end{equation*}
It attempts to score the correct class above all other classes with a
margin of at least 1.

\subsubsection{Log loss}
The log loss is a common variation of the hinge loss, it can be seen
as a soft version with an infinite margin \cite{lecun2006tutorial}. It
is defined as:
\begin{equation*}
  \loss_{log}(\hat{\vect{y}}, \vect{y}) =
  \log(1+\exp(-(\hat{\vect{y}}_t-\hat{\vect{y}}_k)). 
\end{equation*}

\subsubsection{Binary cross-entropy loss}
The binary cross-entropy loss, also called logistic loss, is used in
binary classification with conditional probability outputs. We have a
set of two target classes labeled with $y\in\{0,1\}$. The classifier's
output $\tilde{y}$ is transformed using the sigmoid (also logistic)
function $\sigma(x)=1/(1+e^{-x})$ to the range $[0,1]$. It can be
interpreted as the conditional probability
$\hat{y}=\sigma(\tilde{y})=\prob(y=1|\vect{x})$. The prediction rule is:
\begin{equation*}
  \begin{cases}
    0\quad\hat{y}<0.5,\\
    1\quad\hat{y}\geq 0.5.
  \end{cases}
\end{equation*}

The network is trained to maximize the log conditional probability for
each training example $(\vect{x},y)$. The logistic loss is defined as:
\begin{equation*}
  \loss_{logistic}(\hat{y},y)=-y\log\hat{y}-(1-y)\log(1-\hat{y}).
\end{equation*}

While the hinge loss is preferred when we require a hard decision
rule, the binary cross-entropy is useful when we want the network to
produce class conditional probability.

\subsubsection{Categorical cross-entropy loss}
The categorical cross-entropy, also called negative log likelihood, is
used when a probabilistic interpretation of the scores is desired. Let
$\vect{y}=y_1,\dots,y_n$ be a vector representing the true multinomial
distribution over the labels $1,\dots,n$, and let
$\hat{\vect{y}}=\hat{y}_1,\dots,\hat{y}_n$ be the classifier's output
transformed by a \emph{softmax} function:
\begin{equation*}
  softmax(\vect{x})_i=\frac{e^{x_i}}{\sum_j e^x_j}.
\end{equation*}
$\hat{\vect{y}}$ represents the class membership conditional
distribution $\hat{y}_i=\prob(y=i|\vect{x})$. The categorical
cross-entropy loss measures the dissimilarity between the true label 
distribution $\vect{y}$ and the predicted label distribution
$\hat{\vect{y}}$. It is defined:
\begin{equation*}
  \loss_{cross-entropy}(\hat{\vect{y}},\vect{y})=-\sum_i y_i\log(\hat{y}_i).
\end{equation*}

For hard classification problems in which each training example has a
single correct class assignment, $\vect{y}$ is the one-hot vector of
the true class. In such cases the cross-entropy loss can be simplified
to:
\begin{equation*}
  \loss_{cross-entropy}(\hat{\vect{y}},\vect{y}) = -\log(\hat{\vect{y}}_t),
\end{equation*}
where $t$ is the correct class.

\subsection{Regularization}
The attempt to minimize the loss with \eqref{eq:lossmin} may result
in overfitting the training data, i.e. the model loses the capability
to generalize to new data and the loss evaluates poorly on new data
that is not present in $\mathcal{D}$. To counter that, we often pose
soft restrictions on the form of the solution. This is done using a
\emph{regularization}
function $R(\theta)$ over the parameters returning a scalar that
reflect their \emph{complexity}. This is equivalent to introduce a
restriction over the hypothesis space, as seen in
\cref{sec:supervisedTheory}. We want to keep the model complexity low,
Hence we add the regularizer to \eqref{eq:lossmin} and we let the
optimization problem to balance between low loss and low complexity:
\begin{equation}\label{eq:lossminReg}
  \hat\theta=\argmin_\theta\left\{\frac{1}{n}\sum_{i=1}^n\loss(f(\vect{x}_i;\theta),\vect{y}_i)+\lambda
    R(\theta)\right\}.
\end{equation}

Different combinations of loss function and regularization criteria
result in different learning algorithms with different inductive
biases.

Essentially, regularizers work penalizing high values for \ac{ann}
weights, thus avoiding that the network concentrates only on few
features. In \eqref{eq:lossminReg}, $R$ is applied to all the
parameters, in practice it can be applied on the single layers.

We proceed to describe common regularization functions.

\subsubsection{$L_2$ regularization}
In $L_2$ regularization, also called \emph{gaussian prior} or
\emph{weight decay}, $R$ takes the form of the squared $L_2$ norm
of the parameters:
\begin{equation*}
  R_{L_2}(\matr{W})=||\matr{W}||_2^2=\sum_{i,j}(\matr{W}_{i,j})^2.
\end{equation*}

$L_2$ regularization penalizes high parameters, but when their values
become near to $0$ their effect is negligible.

\subsubsection{$L_1$ regularization}
In $L_1$ regularization, also called \emph{sparse prior} or
\emph{lasso}, $R$ takes the form of the $L_1$ norm of the parameters:
\begin{equation*}
  R_{L_1}(\matr{W})=||\matr{W}||_1=\sum_{i,j}|\matr{W}_{i,j}|.
\end{equation*}

In contrast to $L_2$, $L_1$ regularization penalizes uniformly low and
high parameters. This has the effect of encouraging a sparse solution
\cite{tibshirani1996regression}.

\subsubsection{Elastic-net}
The elastic-net regularizer \cite{zou2005regularization} combines both
$L_1$ and $L_2$ regularization:
\begin{equation*}
  R_{elastic-net}(\matr{W})=\lambda_1R_{L_1}(\matr{W})+\lambda_2R_{L_2}(\matr{W}).
\end{equation*}

\subsubsection{Dropout}
Similarly to other regularization techniques, \emph{dropout} method
\cite{hinton2012improving} is designed to prevent the network from
learning on specific weights. It works by randomly setting to $0$ the
output of randomly chosen neurons for each sample. The connections
between the dropout technique and other regularizations are
established \cite{wager2013dropout}.
\subsection{\acf{sgd}}
\label{sec:sgd}
To successfully train a model, it is necessary to solve the
optimization problem \eqref{eq:lossmin}. A common solution is to use
gradient based methods. Gradient based methods work by estimating the
error computing the loss $\loss$ over the training set and then
calculating the 
gradients of the error respect to the parameters $\theta$. Once the
gradient is computed, the values of the weights are moved in the
opposite direction of the gradient.

\begin{algorithm}
  \caption{\acf{sgd}.}\label{alg:sgd}
  \SetKwInput{Input}{Input}
  \Input{
    Parametrized function $f(\vect{x};\theta)$\\
    Training set $\mathcal{D}(\vect{x}_{1:n},\vect{y}_{1:n})$
  }
  \While{stopping criteria not met}{
    Sample training example $\vect{x}_i,\vect{y}_i$\;
    Compute $\loss(f(\vect{x}_i;\theta),\vect{y}_i)$\;
    $\hat{\vect{g}}\rightarrow$ gradients of
    $\loss(f(\vect{x}_i;\theta),\vect{y}_i)$ w.r.t. $\theta$\;
    $\theta\rightarrow\theta-\nu_t\hat{\vect{g}}$
  }
\end{algorithm}
\acf{sgd} is a general
optimization algorithm that is profitably used in training \ac{ann}
\cite{bottou2012stochastic,lecun1995convolutional}.
The hyperparameter $\nu_t$ is the learning rate. It can either be
fixed throughout the learning process, or decay as a function of the
time step $t$. Adam is a widely used algorithm that adaptively change
the learning rate \cite{kingma2014adam}. The error calculated in
\cref{alg:sgd} is based on only one example. This may result in
an inaccurate gradient calculation. A common way to reduce the noise of
this inaccuracy is to estimate the error and the gradients on a sample
of $m$ examples. This is called minibatch \ac{sgd}.

\subsection{\acf{rnn}}
\acp{rnn} are kinds of \acp{ann} specialized for sequences. They
exhibit an
internal state $\vect{h}$ that changes during the training and that
recursively depends on the state of the previous phase. Precisely we
have \cref{eq:rnnState}:
\begin{equation}\label{eq:rnnState}
  \vect{h}^{(t)} = f(\vect{h}^{(t-1)}, \vect{x}^{(t)}; \vect{\theta}),
\end{equation}
where $\vect{h}$ is the state vector, $\vect{x}$ is the input vector
and $\vect{\theta}$ are the parameters of the state-function
$f$. The index $t$ indicates 
the iteration number and can be interpreted as a discrete time or,
more in general, as the progressive number of the sequence that is
presented as
input.

\begin{figure}
  \centering
  % \subfigure[]{
  \subfloat[\label{fig:rnn1}]{
    \begin{tikzpicture}[x=1.5cm, y=1.5cm]
      \node [] (input) at (0,0) {$\vect{x}$};

      \node [layer] (state) at (0,1) {$\vect{h}$};
      \node [delay] (delay) at (1,1) {};

      \node [] (output) at (0,2) {$\vect{y}$};
      
      \draw [arrow] (input) -- (state);
      \draw [arrow] (state) -- (output);
      \draw (state) edge[arrow, bend left=60] (delay.north);
      \draw (delay.south) edge[arrow, bend left=60] (state);
    \end{tikzpicture}
  }\hfill
  % \subfigure[]{
  \subfloat[\label{fig:rnn2}]{
    \begin{tikzpicture}[x=1.5cm, y=1.5cm]
      \foreach \l [count=\i] in {t-1,t,t+1}{
        \node [] (input-\i) at (\i*1.2,0) {$\vect{x}^{(\l)}$};
        \node [layer] (state-\i) at (\i*1.2,1) {$\vect{h}^{(\l)}$};
        \node [] (output-\i) at (\i*1.2,2) {$\vect{y}^{(\l)}$};

        \draw [arrow] (input-\i) -- (state-\i);
        \draw [arrow] (state-\i) -- (output-\i);
      }
      \node [layer, dashed] (state-l) at (0,1) {$\vect{h}^{(t-2)}$};
      \node [layer, dashed] (state-r) at (4*1.2,1) {$\vect{h}^{(t+2)}$};

      \draw [arrow] (state-l) -- (state-1);
      \draw [arrow] (state-1) -- (state-2);
      \draw [arrow] (state-2) -- (state-3);
      \draw [arrow] (state-3) -- (state-r);
    \end{tikzpicture}
  }
  \caption{\acs{rnn}, folded (a) and unfolded (b) models.}
  \label{fig:rnn}
\end{figure}
In order to express a compact visualization of \acp{rnn}, it is
possible to use the computational graph in \cref{fig:rnn1} where the
black box is a 
delay of one iteration. An extract of the unfolding of the computation
graph is 
shown in \cref{fig:rnn2}. 

\begin{figure}
  \centering
  \begin{tikzpicture}[x=1.5cm, y=1.5cm]
    \foreach \l [count=\i] in {t,t+1,missing,\tau}{
      \ifthenelse{\equal{\l}{missing}}{
        \node [vmissing] (state-\i) at (\i*1.2,1) {};
        \node [hmissing] (state-\i) at (\i*1.2,0) {};
      }{
        \node [] (input-\i) at (\i*1.2,0) {$\vect{x}^{(\l)}$};
        \node [layer] (state-\i) at (\i*1.2,1) {$\vect{h}^{(\l)}$};
        \draw [arrow] (input-\i) -- (state-\i);
        \ifthenelse{\equal{\l}{\tau}}{
          \node [] (output-\i) at (\i*1.2,2) {$\vect{y}^{(\l)}$};
          \draw [arrow] (state-\i) -- (output-\i);
        }
      }
    }

    \node [layer, dashed] (state-l) at (0,1) {$\vect{h}^{(t-1)}$};
    \node [layer, dashed] (state-r) at (5*1.2,1) {$\vect{h}^{(\tau+1)}$};

    \draw [arrow] (state-l) -- (state-1);
    \draw [arrow] (state-1) -- (state-2);
    \draw [arrow,dashed] (state-2) -- ++(0.8,0);
    \draw [arrowInverse,dashed] (state-4) -- ++(-0.8,0);
    \draw [arrow] (state-4) -- (state-r);
  \end{tikzpicture}
  \caption{\ac{rnn} with single output after a sequence of inputs.}
  \label{fig:rnnSO}
\end{figure}
The model in \cref{fig:rnn} provides an output $\vect{y}^{(t)}$ for every
input $\vect{x}^{(t)}$. An alternative is the model of
\cref{fig:rnnSO} where an output $\vect{y}^{(\tau)}$ is provided only
after a sequence $\vect{x}^{(t)},\dots,\vect{x}^{(\tau)}$ of inputs.

In order to train a \ac{rnn}, it is sufficient to apply the
backpropagation to the entire unfolded model.

\subsection{\acf{lstm}}
The major drawback of \acp{rnn} is the \emph{vanishing
  gradient} problem during the backpropagation. The gradient
for long-term associations, propagated through many stages, tends to
become zero. A possible solution for this problem is \ac{lstm} \cite{hochreiter1997long}.

\begin{figure}
  \centering
  % \subfigure[]{
  \subfloat[]{
    \begin{tikzpicture}[x=1.5cm, y=1.5cm]
      \node at (1,-0.5) {};

      \node[delay] (delay1) at (1.6,0.8) {};
      \coordinate (input1) at (2.5,0.5);
      \coordinate (input2) at (3.5,0.5);
      \node[] (input01) at (2.5,0) {$in_1$};
      \node[] (input02) at (3.5,0) {$in_m$};
      \node[] at (3,0.2) {$\cdots$};
      \node[every neuron, label={[xshift=-2mm]input}] (inputN) at (1,2) {$t$};
      \node[every neuron, label={[xshift=0.5mm, align=left, font=\tiny] input\\gate}] (inputGate) at (2,2) {$\sigma$};
      \node[every neuron, label={[xshift=3mm, align=left, font=\tiny] forget\\gate}] (forgetGate) at (3,2) {$\sigma$};
      \node[every neuron, label={[xshift=3mm, align=left, font=\tiny] output\\gate}] (outputGate) at (4,2) {$\sigma$};
      \node[operation] (inputTimes) at (1.5, 3) {$\times$};
      \node[operation, label={[xshift=-3mm]state}] (state) at (2, 4) {$+$};
      \node[delay] (delay2) at (2.7,3.7) {};
      \node[operation] (forgetTimes) at (2.5, 3) {$\times$};
      \node[operation] (outputTimes) at (3, 5) {$\times$};
      \coordinate (output1) at (3,5.5);
      \node (output2) at (3,6) {$out_i$};
      \node at (0.7,5.3) {$cell_i$};

      \fill[black] (input1) circle (1mm);
      \fill[black] (input2) circle (1mm);
      \fill[black] (output1) circle (1mm);
      
      \foreach \n in {input1, input2}{
        \foreach \m in {inputN, inputGate, forgetGate, outputGate}{
          \draw[arrow] (\n) -- (\m);
        }
      }
      \foreach \m in {inputGate, forgetGate}{
        \draw[arrow] (delay1) -- (\m);
      }

      \draw[arrow] (inputN) -- (inputTimes);
      \draw[arrow] (inputGate) -- (inputTimes);
      \draw[arrow] (forgetGate) -- (forgetTimes);
      \draw[arrow] (inputTimes) -- (state);
      \draw[arrow] (forgetTimes) -- (state);
      \draw [arrow] (state) ..  controls  (0.15,4) and (0.15,1) ..  (delay1);
      \draw [arrow] (state) ..  controls  (3.8,4.2) and (3.2,3) ..  (outputGate);
      \draw[arrow] (state) -- (delay2);
      \draw[arrow] (delay2) -- (forgetTimes);
      \draw[arrow] (state) -- (outputTimes);
      \draw[arrow] (outputGate) -- (outputTimes);
      \draw[line] (outputTimes) -- (output1);
      \draw[line] (output1) -- (output2);
      \draw[line] (input01) -- (input1);
      \draw[line] (input02) -- (input2);

      \path[border] (0.4,0.5) -- (4.5,0.5) -- (4.5,5.5) -- (0.4,5.5) -- (0.4,0.5); 
      

    \end{tikzpicture}
    \label{fig:lstm1}
  }\hfill
  % \subfigure[]{
  \subfloat[]{
    \begin{tikzpicture}[x=0.8cm, y=0.8cm]
      \node at (1,-2.5) {};

      % \draw[step=1.0,black,thin] (0,3) grid (4,4);
      \draw[step=1.0,black,thin] (0,3) grid (2,4);
      \draw[step=1.0,black,thin] (3,3) grid (4,4);
      \coordinate (input1) at (0.5,3);
      \coordinate (input2) at (1.5,3);
      \coordinate (input3) at (3.5,3);
      \coordinate (inputM1) at (0.5,2.5);
      \coordinate (inputM2) at (1.5,2.5);
      \coordinate (inputM3) at (3.5,2.5);
      \coordinate (output1) at (0.5,4);
      \coordinate (output2) at (1.5,4);
      \coordinate (output3) at (3.5,4);

      \coordinate (outputM) at (2,6);
      \coordinate (outputTM) at (1,6);
      
      \node at (0.5, 3.5) {$cell_1$};
      \node at (1.5, 3.5) {$cell_2$};
      \node at (2.5, 3.5) {$\cdots$};
      \node at (3.5, 3.5) {$cell_n$};
      \coordinate (pass) at (4.5, 3.5);
      \node (inputT) at (1,0) {$\vect{x}$};
      \coordinate (inputTM) at (1,1);
      \node (outputT) at (1,7) {$\vect{y}$};

      \node[delay] (delay) at (4,1) {};

      \foreach \c in {input1, input2, input3, output1, output2, output3}{
        \fill[black] (\c) circle (1mm);
      }

      \foreach \c in {inputM1, inputM2, inputM3}{
        \draw [vectorLine] (delay) ..  controls
        ($0.5*(delay)+0.5*(\c)$) and ($(\c)-(0,0.5)$) ..  (\c);
        \draw [vectorLine] (inputTM) ..  controls
        ($0.5*(inputTM)+0.5*(\c)$) and ($(\c)-(0,0.5)$) ..  (\c);
      }

      \foreach \c in {1, 2, 3}{
        \draw[vectorArrow] (inputM\c) -- ($(input\c)-(0,0.1)$);
      }

      \foreach \c in {output1, output2, output3}{
        \draw [line] (\c) ..  controls  ($(\c)+(0,0.5)$) and
        ($(outputM)-(0,0.5)$) ..  (outputM);
        \draw [line] (\c) ..  controls  ($(\c)+(0,0.5)$) and
        ($(outputTM)-(0,0.5)$) ..  (outputTM);
      }

      %\draw [vectorArrow] (outputM) ..  controls  ($(outputM)+(1,2)$) and ($(delay)+(1,0)$) ..  (delay);
      \draw [vectorArrow]
      (outputM) ..
      controls  ($(outputM)+(1,2)$) and ($(pass)+(0,1)$) ..
      (pass) ..
      controls  ($(pass)-(0,1)$) and ($(delay)+(1,0)$) ..
      (delay);
      \draw[vectorLine] (inputT) -- (inputTM);
      \draw[vectorArrow] (outputTM) -- (outputT);
    \end{tikzpicture}
    \label{fig:lstm2}
  }
  \caption{\ac{lstm} model: detail of memory cell (a), and general
    scheme (b). The black box is a delay of one iteration.}
  \label{fig:lstm}
\end{figure}
The \ac{lstm} model is structured as in \cref{fig:lstm}. It is
equivalent to a
generic \ac{rnn} where the hidden state $\vect{h}$ is a
layer of \emph{memory cells}.

In detail, a single
memory cell (\cref{fig:lstm1}) has four \acf{an}. One is labelled \emph{input}
and processes the cell's 
inputs into an internal state. The other three \acp{an} are labelled as
\emph{gates} and process the cell's inputs together with the previous-phase
state in order to decide:
\begin{itemize}
\item how much of the current input must be learned
  (\emph{input gate});
\item how much of the previous-phase state must be forgotten
  (\emph{forget gate});
\item how much of the state constitutes the output (\emph{output gate}).
\end{itemize}

The memory-cells layer (\cref{fig:lstm2}) is composed of a number of
cells equal to the dimension of the output. Each cell
calculates one dimension of $\vect{y}$. The input $\vect{x}$ is copied
for every cell and, together with the previous-phase
output,
constitutes the input $in_1,\dots,in_m$ of the single cells.

\subsection{\acf{gru}}
\acfp{gru} were introduced by \cite{cho2014learning} as an
alternative to \ac{lstm}. \ac{gru} is also based on gating mechanism,
but it has fewer gates respect to \ac{lstm} and the memory is directly
exposed to the output. One gate $r$ called \emph{reset} is used to control
access to the previous state $h_{t-1}$ and computes a proposed update
$\tilde{h}$. The new 
status $h_t$ is calculated as an interpolation between the previous and the
proposed status, where the proportion of the interpolation is
controlled by the \emph{update} gate $z$.
Formally, at time $t$ the \ac{gru} computes its new state $h_t$ as:
\begin{equation*}
  h_t=(1-z_t)\bigodot h_{t-1}+z_t\bigodot\tilde{h}_t,
\end{equation*}
and the proposed new state $\tilde{h}_t$ as:
\begin{equation*}
  \tilde{h}_t=tanh(W_h x_t+r_t\odot(U_h h_{t-1})+b_h).
\end{equation*}
The update gate $z$ controls how much information from the past should be
keep and how much new information should be have. $z_t$ is updated as:
\begin{equation*}
  z_t=\sigma(W_z x_t+U_z h_{t-1}+bz).
\end{equation*}
The reset gate $r$ controls how much of the past state contributes to
the candidate. It is calculated as:
\begin{equation*}
  r_t=\sigma(W_r x_r+U_r h_{t-1}+b_r).
\end{equation*}

\begin{figure}
  \centering
  % \subfigure[]{
  \subfloat[]{
    \begin{tikzpicture}[x=1.5cm, y=1.5cm]
      \node at (1,-0.5) {};

      \coordinate (input1) at (1,0.5);
      \coordinate (input2) at (2,0.5);
      \coordinate (input3) at (2.4,0.5);
      \coordinate (input4) at (3.4,0.5);
      \node[] (input01) at (1,0) {$x_1$};
      \node[] (input02) at (2,0) {$x_m$};
      \node[] (input03) at (2.4,0) {$h_1$};
      \node[] (input04) at (3.4,0) {$h_n$};
      \node[] at (1.5,0.2) {$\cdots$};
      \node[] at (2.9,0.2) {$\cdots$};
      \node[every neuron, label={[xshift=-10mm, yshift=-10mm, align=left, font=\tiny]update\\gate}] (updateGate) at (0.5,2.5) {$\sigma$};
      \node[every neuron, label={[xshift=3mm, align=left, font=\tiny] reset\\gate}] (resetGate) at (4,1.5) {$\sigma$};
      \node[operation] (updateTimesLeft) at (0, 4) {$\times$};
      \node[operation] (updateTimesRight) at (1, 4) {$\times$};
      \node[operation, label={[xshift=10mm,yshift=-10mm]state}] (state) at (0.5, 5) {$+$};
      \node[delay] (delay) at (-0.3,4.7) {};
      \node[operation] (resetTimesLeft) at (2.4, 2.5) {$\times$};
      \node[operation] (resetTimesRight) at (3.4, 2.5) {$\times$};
      \node[every neuron] (candidateState) at (2,3.5) {$t$};
      \coordinate (output1) at (2.2,5.5);
      \node (output2) at (2.2,6) {$out_i$};
      \node at (4.2,5.3) {$cell_i$};

      \fill[black] (input1) circle (1mm);
      \fill[black] (input2) circle (1mm);
      \fill[black] (input3) circle (1mm);
      \fill[black] (input4) circle (1mm);
      \fill[black] (output1) circle (1mm);
      
      \foreach \n in {input1, input2, input3, input4}{
        \foreach \m in {updateGate, resetGate}{
          \draw[arrow] (\n) -- (\m);
        }
      }

      \foreach \n in {input1, input2, resetTimesLeft, resetTimesRight}{
        \draw[arrow] (\n) -- (candidateState);
      }
      
      \draw[arrow] (updateGate) -- (updateTimesLeft) node[draw=none,fill=none,font=\scriptsize,midway,left] {$1-$};
      \draw[arrow] (updateGate) -- (updateTimesRight);
      \draw[arrow] (updateTimesLeft) -- (state);
      \draw[arrow] (updateTimesRight) -- (state);
      %\draw [arrow] (state) ..  controls  (3.8,4.2) and (3.2,3) ..  (resetGate);
      \draw[arrow] (state) -- (delay);
      \draw[arrow] (delay) -- (updateTimesLeft);
      \draw[arrow] (state) -- (output1);
      \draw[arrow] (input3) -- (resetTimesLeft);
      \draw[arrow] (input4) -- (resetTimesRight);
      \draw[arrow] (resetGate) -- (resetTimesLeft);
      \draw[arrow] (resetGate) -- (resetTimesRight);
      \draw[arrow] (candidateState) -- (updateTimesRight);
      \draw[line] (output1) -- (output2);
      \draw[line] (input01) -- (input1);
      \draw[line] (input02) -- (input2);
      \draw[line] (input03) -- (input3);
      \draw[line] (input04) -- (input4);

      \path[border] (-0.6,0.5) -- (4.5,0.5) -- (4.5,5.5) -- (-0.6,5.5) -- (-0.6,0.5); 
      

    \end{tikzpicture}
    \label{fig:gru1}
  }\hfill
  % \subfigure[]{
  \subfloat[]{
    \begin{tikzpicture}[x=0.8cm, y=0.8cm]
      \node at (1,-2.5) {};

      % \draw[step=1.0,black,thin] (0,3) grid (4,4);
      \draw[step=1.0,black,thin] (0,3) grid (2,4);
      \draw[step=1.0,black,thin] (3,3) grid (4,4);
      \coordinate (input1) at (0.5,3);
      \coordinate (input2) at (1.5,3);
      \coordinate (input3) at (3.5,3);
      \coordinate (inputM1) at (0.5,2.5);
      \coordinate (inputM2) at (1.5,2.5);
      \coordinate (inputM3) at (3.5,2.5);
      \coordinate (output1) at (0.5,4);
      \coordinate (output2) at (1.5,4);
      \coordinate (output3) at (3.5,4);

      \coordinate (outputM) at (2,6);
      \coordinate (outputTM) at (1,6);
      
      \node at (0.5, 3.5) {$cell_1$};
      \node at (1.5, 3.5) {$cell_2$};
      \node at (2.5, 3.5) {$\cdots$};
      \node at (3.5, 3.5) {$cell_n$};
      \coordinate (pass) at (4.5, 3.5);
      \node (inputT) at (1,0) {$\vect{x}$};
      \coordinate (inputTM) at (1,1);
      \node (outputT) at (1,7) {$\vect{y}$};

      \node[delay] (delay) at (4,1) {};

      \foreach \c in {input1, input2, input3, output1, output2, output3}{
        \fill[black] (\c) circle (1mm);
      }

      \foreach \c in {inputM1, inputM2, inputM3}{
        \draw [vectorLine] (delay) ..  controls
        ($0.5*(delay)+0.5*(\c)$) and ($(\c)-(0,0.5)$) ..  (\c);
        \draw [vectorLine] (inputTM) ..  controls
        ($0.5*(inputTM)+0.5*(\c)$) and ($(\c)-(0,0.5)$) ..  (\c);
      }

      \foreach \c in {1, 2, 3}{
        \draw[vectorArrow] (inputM\c) -- ($(input\c)-(0,0.1)$);
      }

      \foreach \c in {output1, output2, output3}{
        \draw [line] (\c) ..  controls  ($(\c)+(0,0.5)$) and
        ($(outputM)-(0,0.5)$) ..  (outputM);
        \draw [line] (\c) ..  controls  ($(\c)+(0,0.5)$) and
        ($(outputTM)-(0,0.5)$) ..  (outputTM);
      }

      %\draw [vectorArrow] (outputM) ..  controls  ($(outputM)+(1,2)$) and ($(delay)+(1,0)$) ..  (delay);
      \draw [vectorArrow]
      (outputM) ..
      controls  ($(outputM)+(1,2)$) and ($(pass)+(0,1)$) ..
      (pass) ..
      controls  ($(pass)-(0,1)$) and ($(delay)+(1,0)$) ..
      (delay);
      \draw[vectorLine] (inputT) -- (inputTM);
      \draw[vectorArrow] (outputTM) -- (outputT);
    \end{tikzpicture}
    \label{fig:gru2}
  }
  \caption{\ac{gru} model: detail of memory cell (a), and general
    scheme (b). The black box is a delay of one iteration.}
  \label{fig:gru}
\end{figure}

The choice between \ac{lstm} and \ac{gru} is context dependent, in
certain applications \ac{lstm} gives better performances, in other
applications \ac{gru} does \cite{yin2017comparative}. 




%%% Local Variables:
%%% mode: latex
%%% TeX-master: "thesis"
%%% End:
