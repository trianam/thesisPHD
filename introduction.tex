\chapter{Introduction}
\label{ch:introduction}

\section{Machine Learning Theory}
\ac{ml} is a branch of \ac{ai}
where the focus is to develop systems that learn from the
data. Machine Learning is a field that is located in the intersection
of different disciplines:
\begin{description}
\item[statistics] deal with the uncertainty of the world and, as we
  will see, ML can be framed as a probability estimation;
\item[data science] is the science that interpretes and study the
  information contained in the data and how can be used;
\item[optimization theory] is the mathematical branch that define
  methods to optimize functions, ML can be seen as an optimization
  problem;
\item[computer science] is the science that studies algorithms and
  their complexity;
\item[computer engineering] focuses in the development of efficient
  software.
\end{description}

In order to learn we must provide some data. The data can be of
different kinds. We can denote three different types of \ac{ml}, based
on the type of data and problem that we need to resolve: supervised,
unsupervised, and reinforcement learning.

\subsection{Supervised Learning}
In supervised learning we posses a dataset $S$ of samples $\vect{x}_i$
each one labeled with $\vect{y}_i$:
\begin{equation*}
  S=\left((\vect{x}_1,\vect{y}_1),\dots,(\vect{x}_n,\vect{y}_n)\right),
\end{equation*}
where each $\vect{x}_i\in\XSet$ and $\vect{y}_i\in\YSet$. Usually the
data is prepared such that $\XSet$ is a space of integer or real
tensors of certain dimensionality, and $\YSet$ is a space of integer
vectors for classification tasks and real vectors for regression
tasks. In binary classification tasks $\YSet={0,1}$, in multilabel
classification $\YSet={0,1}^k$ with $k$ possible labels, and in
multiclass classification $\YSet={0,1}^k$ with only one value equal to
$1$ and the rest $0$. The samples $\vect{x}_i$ of a dataset must be
drawn from the same unknown distribution $\vect{x}_i\sim\dist{D}$. The
relationship between the samples and the labels is defined by an unknown
labeling function $f:\XSet\rightarrow\YSet$ such that for each sample
$\vect{y}_i=f(\vect{x}_i)$.

A learning algorithm receives as input a training set $S$, and should
output a predictor $h_S:\XSet\rightarrow\YSet$ that minimize the
\emph{prediction error}. For a generic predictor $h$, the prediction
error is:
\begin{equation}\label{predictionError}
  L_{\dist{D},f}(h)\define\prob_{\vect{x}\sim\dist{D}}[h(\vect{x})\neq
  f(\vect{x})]\define\dist{D}(\{\vect{x}:h(\vect{x})\neq
  f(\vect{x})\}),
\end{equation}
where for $A\subset\XSet$, the probability $\dist{D}$ assign a
likelihood $\dist{D}(A)$ of observing a value $\vect{x}\in A$. Given
that both $\dist{D}$ and $f$ are unknown, to find $h_S$ the learning
algorithm minimizes the \emph{empirical prediction error} (or
empirical risk):
\begin{equation}\label{empiricalError}
  L_S(h)\define\frac{|i\in\{1,\dots,n\}:h(\vect{x}_i)\neq\vect{y}_i|}{n}.
\end{equation}
This learning paradigm of finding $h_S$ is called \ac{erm}.

\ac{erm} rule might lead to \emph{overfitting} if not restricted. A predictor
can performs well over the training set $S$ --- having a low empirical
error --- but it can performs badly over the entire distribution
$\dist{D}$ with an high prediction error. A solution is to apply
\ac{erm} over a restricted search space. We denote with $\HSet$ the
\emph{hypothesis class} of the possible predictors
$h\in{HSet}:\XSet\rightarrow\YSet$. For a given class $\HSet$ and a
training sample $S$, the $ERM_\HSet$ learner uses the \ac{erm} rule to
choose a predictor $h_S\in\HSet$ with the lowest possible empirical
error over $S$:
\begin{equation*}
  h_S=ERM_\HSet(S)\in \argmin_{h\in\HSet} L_S(h).
\end{equation*}

We induce an \emph{inductive bias} introducing restrictions over
$\HSet$. Intuitively, choosing a more restricted hypothesis class
better protect us against overfitting but at the same time might cause
a stronger bias.

\subsubsection{Finite hypothesis classes}
The simplest kind of restriction on $\HSet$ is
imposing an upper bound on its dimension. We make the two following
assumptions: 
\begin{definition}\label{realizability}
  (Realizability assumption). $\exists h^*\in\HSet : L_{(\dist{D},f)}(h^*)=0$.
\end{definition}
\begin{definition}
  (\ac{iid} examples). $S\sim\dist{D}^m$, with $m=|S|$ and $\dist{D}^m$ the
  probability over $m$-tuples induced by applying $\dist{D}$ to pick
  each element of the tuple independently from the others elements of
  the same tuple.
\end{definition}

$L_{(\dist{D},f)}(h_S)$ depends on the choice of $S$, that is a
random variable. To address the probability to sample $S$ such that
the prediction error is not too large, we denote with $\delta$ the
probability of getting a nonrepresentative sample, and $(1-\delta)$
the \emph{confidence parameter} of the prediction. Furthermore we
introduce the \emph{accuracy parameter} $\varepsilon$ of the quality
of the prediction. $L_{(\dist{D},f)}(h_S)>\varepsilon$ represents
a failure of the learner.
We are interested in upper bounding the probability to sample
$m$-tuple that lead to the learner failure. If $S|_x=(\vect{x}_1,\dots,\vect{x}_m)$
are instances of the training set, we want to upper bound
\begin{equation*}
  \dist{D}^m(\{S|_x:L_{(\dist{D},f)}(h_S)>\varepsilon\}).
\end{equation*}
It is possible to prove that for a finite hypothesis class $\HSet$:
\begin{equation*}
  \dist{D}^m(\{S|_x:L_{(\dist{D},f)}(h_S)>\varepsilon\}) \leq
  |\HSet|e^{-\varepsilon m},
\end{equation*}
and for $m$ an integer that satisfies
\begin{equation*}
  m\geq\frac{\log(|\HSet|/\delta)}{\varepsilon},
\end{equation*}
then for any $f$ and $\dist{D}$, with probability at least $1-\sigma$,
for every $h_S$ holds:
\begin{equation*}
  L_{(\dist{D},f)}(h_S)\leq\varepsilon.
\end{equation*}
This means that for sufficiently large $m$, the $ERM_\HSet$ rule over
a finite hypothesis class will be probably (with confidence
$1-\delta$) approximately (up to an error $\varepsilon$) correct.

\subsubsection{\ac{pac} learning}
\begin{definition}\label{pacLearnability}
A hypothesis class $\HSet$ is \ac{pac} learnable if there exist a
function $m_\HSet:(0,1)^2\rightarrow\NSet$ and a learning algorithm
with the following property: for every $\varepsilon,\sigma\in(0,1)$,
for every $\dist{D}$ and $f$, if \cref{realizability} holds, then when
running the algorithm on $m\geq m_\HSet(\varepsilon,\sigma)$ \ac{iid}
examples, it returns a hypothesis $h$ such that
$L_{(\dist{D},f)}(h)\leq\varepsilon$, with probability of at least
$1-\sigma$ over the choice of the examples.
\end{definition}

The function $m_\HSet$ determines the \emph{sample complexity} of
learning $\HSet$, that is the number of samples to guarantee a
\ac{pac} solution. The finite hypothesis class of
is \ac{pac} learnable with sample complexity
\begin{equation*}
  m_\HSet(\varepsilon,\delta)\leq\left\lceil\frac{\log(|\HSet|/\delta}{\varepsilon}\right\rceil
\end{equation*}

\subsubsection{Agnostic \ac{pac} learning}
To develop a more realistic model, we relax the \cref{realizability}
and we substitute the notion of $\dist{D}$ over $\XSet$ and
$f:\XSet\rightarrow\YSet$ with a single data generating
distribution. From now, $\dist{D}$ is defined as a probability
distribution over $\XSet\times\YSet$, and can be seen as being
composed by a \emph{marginal distribution} $\dist{D}_x$ over $\XSet$
and a \emph{conditional probability} $\dist{D}((\vect{x},\vect{y})|\vect{x})$ over label
for each example. The true error \cref{predictionError} is redefined as:
\begin{equation*}
  L_{\dist{D},f}(h)\define\prob_{(\vect{x},\vect{y})\sim\dist{D}}[h(\vect{x})\neq
  f(\vect{x})]\define\dist{D}(\{(\vect{x},\vect{y}):h(\vect{x})\neq
  f(\vect{x})\}),  
\end{equation*}
while the empirical error \cref{empiricalError} remains the same.

We introduce the concept of \emph{loss function} as a function
$\loss:\HSet\times(\XSet\times\YSet)\rightarrow\RSet^+$, and the true
and empirical risks are defined in term of $\loss$ as:
\begin{align*}
  L_\dist{D}(h)&\define\expect_{(\vect{x},\vect{y})\sim\dist{D}}[\loss(h,(\vect{x},\vect{y}))],\\
  L_S(h)&\define\frac{1}{m}\sum_{i=2}^m\loss(h,(\vect{x}_i,\vect{y}_i))].
\end{align*}
Two common losses are:
\paragraph{0-1 loss}
\begin{equation*}
  \loss_{0-1}(h,(\vect{x},\vect{y}))\define\begin{cases}
    0\quad\mathrm{if}\ h(\vect{x}) = \vect{y},\\
    1\quad\mathrm{if}\ h(\vect{x}) \neq \vect{y}.
  \end{cases}
\end{equation*}
\paragraph{Square loss}
\begin{equation*}
  \loss_{sq}(h,(\vect{x},\vect{y}))\define(h(\vect{x})-\vect{y})^2.
\end{equation*}

Is it possible now to define the concept of agnostic \ac{pac}
learnability.
\begin{definition}\label{agnosticPacLearnability}
A hypothesis class $\HSet$ is agnostic \ac{pac}
learnable, respect $(\XSet\times\YSet)$ and $\loss$, if there exist a
function $m_\HSet:(0,1)^2\rightarrow\NSet$ and a learning algorithm
with the following property: for every $\varepsilon,\delta\in(0,1)$
and for every $\dist{D}$, when running the algorithm on $m\geq
m_\HSet(\varepsilon,\delta)$ \ac{iid} examples, it returns $h$ such
that
$L_\dist{D}(h)\leq\min_{h'\in\HSet}L_\dist{D}(h')+\varepsilon$. 
\end{definition}

Contrarily to \cref{pacLearnability}, \cref{agnosticPacLearnability}
cannot guarantee an arbitrarily small error. It still guarantee that
a learner can have success if its error is not too much larger than
the one of the best achievable predictor for class $\HSet$.

The \ac{erm} learning paradigm works by finding an hypothesis that
minimize the empirical 
risk. This means that an $h$ that minimizes the empirical risk needs
to be a good true risk minimizer also. We need that uniformly over all
hypothesis, the empirical risk needs to be close to the real risk. To
asses this we introduce the concept of
\emph{$\varepsilon$-representative sample}: 
\begin{definition}
  ($\varepsilon$-representative sample). A training set $S$ is called
  $\varepsilon$-representative with regard to $\HSet$, $\loss$, and
  $\dist{D}$, if
  \begin{equation*}
    \forall h\in\HSet,\quad |L_S(h)-L_\dist{D}(h)|\leq\varepsilon.
  \end{equation*}
\end{definition}
It is possible to prove that if the sample is
$\frac{\varepsilon}{2}$-representative, then the \ac{erm} learning
rule is guaranteed to return a good hypothesis. Formally:
\begin{lemma}\label{ermgood}
  Assuming $S$ is $\frac{\varepsilon}{2}$-representative, then any
  output $ERM_\HSet(S)=h_S\in\argmin_{h\in\HSet}L_S(h)$ satisfy:
  \begin{equation*}
    L_\dist{D}(h_S)\leq\min_{h\in\HSet}L_\dist{D}(h)+\varepsilon.
  \end{equation*}
\end{lemma}

To ensure that the \ac{erm} rule is an agnostic \ac{pac} learner, it
suffices to show that with probability of at least $1-\delta$ over the
choice of $S$, it will be an $\varepsilon$-representative training
set. Formally, given the property:
\begin{definition}
  (Uniform Convergence). $\HSet$ has the uniform convergence property
  if there exists a function $m_\HSet^{UC}:(0,1)^2\rightarrow\NSet$
  such that for every $\varepsilon,\delta\in(0,1)$ and for every
  $\dist{D}$, if $S$ is a sample of $m\geq
  m_\HSet^{UC}(\varepsilon,\delta)$ examples drawn \ac{iid} from
  $\dist{D}$, then $S$ is $\varepsilon$-representative with
  probability of at least $1-\delta$.
\end{definition}
We have the corollary of \cref{ermgood}:
\begin{corollary}
  If a class $\HSet$ has the uniform convergence property, then the
  class is agnostically \ac{pac} learnable with sample complexity
  $m_\HSet(\varepsilon,\delta)\leq
  m_\HSet^{UC}(\varepsilon/2,\delta)$. In that case the $ERM_\HSet$
  paradigm is a successful agnostic \ac{pac} learner for $\HSet$.
\end{corollary}

It is possible to prove (we skip the proof here) that the uniform convergence
holds for a finite
hypothesis class, and then that every finite hypothesis class is
agnostic \ac{pac} learnable using \ac{erm} algorithm with sample
complexity
\begin{equation*}
  m_\HSet(\varepsilon, \delta)\leq
  m_\HSet^{UC}\left(\frac{\varepsilon}{2},\delta\right)\leq\left\lceil\frac{2\log\left(\frac{2|\HSet|}{\delta}\right)}{\varepsilon^2}\right\rceil.
\end{equation*}

\subsubsection{Bias-complexity trade-off}
Choosing a hypothesis class $\HSet$ represents a prior
knowledge related to the data distribution $\dist{D}$. One can argue
if this prior knowledge is really necessary, or 
if it is possible to have a learning algorithm $A$ and a size $m$ such
that for every $\dist{D}$, if $A$ receives $m$ \ac{iid} samples from
$\dist{D}$, it outputs a predictor $h$ that have a low risk with high
chance. The \emph{no-free-lunch} theorem proves that having such a
universal learner is impossible:

One can avoid the limits imposed by the no-free-lunch theorem using
the prior knowledge of the learning task to restrict the hypothesis
class $\HSet$. To choose a good hypothesis class we need to set a
large enough class such that it includes the hypothesis with no error
(for \ac{pac} learnability), or at least that the smallest error
achievable by a hypothesis from the class is small enough (for
agnostic \ac{pac} learnability). On the other hand we cannot choose
the richest class. We can decompose the error of an $ERM_\HSet$
hypothesis $h_S$ into the component:
\begin{equation*}
  L_\dist{D}(h_S)=\varepsilon_{app}+\varepsilon_{est}.
\end{equation*}
where
\begin{equation*}
  \varepsilon_{app}=\min_{h\in\HSet}L_\dist{D}(h)
\end{equation*}
is the \emph{approximation error} that measures how much inductive
bias we 
introduce restricting $\HSet$ and do not depends on the sample size.
\begin{equation*}
  \varepsilon_{est}=L_\dist{D}(h_S)-\varepsilon_{app}
\end{equation*}
is the \emph{estimation error} that depends on the training set size
and on the complexity of $\HSet$.

To minimize the total risk, we face a \emph{bias-complexity
  tradeoff}. Choosing $\HSet$ to be rich, decreases the approximation
error, but increases the estimation error because it may lead to
overfitting. Choosing a small $\HSet$ reduces the estimation error,
but can increases the approximation error, in other words it might
lead to \emph{underfitting}.

\subsubsection{\ac{vcd}}
The finitess of $\HSet$ is a sufficient condition for learnability,
but it is not a necessary condition. A property called \ac{vcd} gives
a correct characterization of learnability. Considering only binary
classifiers where $\YSet=\{0,1\}$, to define \ac{vcd}, we
must define before the concept of \emph{class restriction} and
\emph{shattering}:
\begin{definition}
  (Restriction of $\HSet$ to $C$). $\HSet$ is a class of function
  $h:\XSet\rightarrow\{0,1\}$, and
    $C=\{c_1,\dots,c_m\}\subset\XSet$. The restriction of $\HSet$ to
    $C$ is the set of function from $C$ to $\{0,1\}$ that can be
    derived from $\HSet$:
    \begin{equation*}
      \HSet_C=\{(h(c_1),\dots,h(c_m)):h\in\HSet\},
    \end{equation*}
    where each function is represented as a vector in
    $\{0,1\}^{|C|}$.
\end{definition}
\begin{definition}
  (Shattering). A hypothesis class $\HSet$ shatters a finite set
  $C\subset\XSet$, if $\HSet_C$ is the set of all functions from $C$
  to $\{0,1\}$:
  \begin{equation*}
    |\HSet_C|=2^{|C|}.
  \end{equation*}
\end{definition}

Relating to the free-lunch-theorem, whenever some set $C$ is shattered
by $\HSet$, an adversary can 
construct a distribution over $C$ based on any function from $C$ to
$\{0,1\}$ maintaining the realizability assumption. Thus, for any
learning algorithm $A$ there exist a distribution $\dist{D}$ and a
predictor $h$
(both constructed from an adversary) such that $L_D(h)=0$, but with a
sufficiently high probability over the choice of $S$,
$L_\dist{D}(A(S))\geq\varepsilon$. The \ac{vcd} is defined:
\begin{definition}
  (\ac{vcd}). The \ac{vcd} $VCdim(\HSet)$ of an hypothesis class
  $\HSet$, is the maximal size of a set $C\subset\XSet$ that can be
  shattered by $\HSet$. If $\HSet$ can shatter arbitrarily large sets,
  then $VCdim(\HSet)=\infty$.
\end{definition}

If a certain hypothesis class $\HSet$ has infinite \ac{vcd}, then
$\HSet$ is not \ac{pac} learnable. Conversely, a finite \ac{vcd}
guarantees learnability. To prove that $VCdim(\HSet)=d$ finite, we
need to show
\begin{itemize}
\item $\exists C$ such that $|C|=d$ and it is shattered by $\HSet$;
\item $\forall C$ such that $|C|=d+1$, $C$ is not shattered by
  $\HSet$. 
\end{itemize}

\subsubsection{\ac{srm}}
Until now the prior knowledge was encoded specifying a hypothesis
class $\HSet$ that is believed to include a good predictor for the
learning task. Another way to express the prior knowledge is by
specifying preferences over hypotheses within $\HSet$. The \ac{srm}
implement this assuming $\HSet=\bigcup_{n\in\NSet}\HSet_n$ and
defining a weight function $w:\NSet\rightarrow[0,1]$ that assign a
preference for each class $\HSet_n$. The \ac{srm} rule follows a
\emph{bound minimization} approach. The goal is to find a hypothesis
that minimizes a certain upper bound on the true risk.

\section{Neural Networks}
In our work we used a specific kind of \ac{rnn}: \ac{lstm}.
\acp{rnn} are a class of \acp{ann} where the connections are not only
sequential from one layer to the subsequent, but instead they form
loops. 

\subsection{\acf{ann}}
\begin{figure}
  \centering
  \begin{tikzpicture}[x=1.5cm, y=1.5cm]
    \node [] (input-1) at (0,0.5) {$x_1$};
    \node [] (input-2) at (0,0) {$x_2$};
    \node [] (input-3) at (0,-0.5) {$x_3$};
    \node [] (input-missing) at (0,-0.9) {$\vdots$};
    \node [] (input-n) at (0,-1.4) {$x_n$};

    \node [every neuron] (neuron) at (2,-0.5) {$\Sigma$};
    \node [bias] (bias) at (2,-1.3) {$b$};

    \node [activation] (act) at (3,-0.5) {$f$};
    \node [] (output) at (4,-0.5) {$y$};
    
    \foreach \i in {1,...,3,n}
    \draw [arrow] (input-\i) -- (neuron)
    node [above=-0.05, pos=0.3, sloped] {$w_\i$};

    \draw [arrow] (bias) -- (neuron);

    \draw [arrow] (neuron) -- (act);
    \draw [arrow] (act) -- (output);
  \end{tikzpicture}
  \caption{Artificial neuron.}
  \label{fig:neuron}
\end{figure}
An \ac{ann} is a model that performs elaboration in a way that
mimics the brain functioning. The base unit is the \ac{an}, also called
perceptron, of
\cref{fig:neuron}. It performs the weighted sum of the inputs
$x_1,\dots,x_n$, shifted by a bias $b$, followed by an activation
function $f$. If we add a dummy input $x_0=1$ and rename the bias
$b=w_0$, we can express the computation of the \ac{an} as in
\cref{eq:anComp}:
\begin{equation}\label{eq:anComp}
  y = f(\sum_{i=0}^n w_i x_i).
\end{equation}

The activation function $f$ can be of different types, the most common are:
\begin{itemize}
\item Softmax;
\item ReLU;
\item TanH;
\item Sigmoid;
\item Linear;
\end{itemize}

\begin{figure}
  \centering
  \begin{tikzpicture}[x=1.5cm, y=1.5cm]
    \foreach \m/\l [count=\y] in {1,2,3,missing,4}
    \node [every neuron/.try, neuron \m/.try] (input-\m) at (0,2.5-\y) {};

    \foreach \m [count=\y] in {1,missing,2}
    \node [every neuron/.try, neuron \m/.try ] (hidden-\m) at (2,2-\y*1.25) {};

    \foreach \m [count=\y] in {1,missing,2}
    \node [every neuron/.try, neuron \m/.try ] (output-\m) at (4,1.5-\y) {};

    \foreach \l [count=\i] in {1,2,3,n}
    \node[] (x-\i) at ($(input-\i)-(1,0)$) {$x_\l$};

    \foreach \l [count=\i] in {1,2,3,n}
    \draw [arrow] (x-\i) -- (input-\i);
    %\draw [arrowInverse] (input-\i) -- ++(-1,0)
    %node [above, midway] {$I_\l$};

    %\foreach \l [count=\i] in {1,n}
    %\node [above] at (hidden-\i.north) {$H_\l$};

    \foreach \l [count=\i] in {1,m}
    \node[] (y-\i) at ($(output-\i)+(1,0)$) {$y_\l$};
    
    \foreach \l [count=\i] in {1,n}
    \draw [arrow] (output-\i) -- (y-\i);
    %\draw [arrow] (output-\i) -- ++(1,0)
    %node [above, midway] {$O_\l$};

    \foreach \i in {1,...,4}
    \foreach \j in {1,...,2}
    \draw [arrow] (input-\i) -- (hidden-\j);

    \foreach \i in {1,...,2}
    \foreach \j in {1,...,2}
    \draw [arrow] (hidden-\i) -- (output-\j);

    \foreach \l [count=\x from 0] in {Input, Hidden, Ouput}
    \node [align=center, above] at (\x*2,2) {\l \\ layer};

  \end{tikzpicture}
  \caption{\acf{mlp} with one hidden layer.}
  \label{fig:ann}
\end{figure}
\acp{an} are organized in network structures. The basic layout of
an  \ac{ann} is the \acf{mlp} structured in layers like in
\cref{fig:ann}. Each \ac{an} of each layer is connected to all the
outputs of the previous layer. The first layer is connected to the
inputs of the \ac{ann} and the output of the last layer is also the
output of the network. The execution of the \ac{mlp} is feed forward:
\begin{enumerate}
\item the input values $x_{0,1},\dots x_{0,n}$ are presented to the
  network and it is the input of the first layer;
\item  the computation is 
  carried one layer at a time, where each neuron $i$ of the layer $l$
  calculates the value $y_{l,i}$ of the intermediate output
  $y_{l,1},\dots,y_{l,m}$;
\item the intermediate output $y_{l,1},\dots,y_{l,m}$ of the layer $l$
  becomes the 
  input $x_{l+1,1},\dots,x_{l+1,m}$ of the subsequent layer $l+1$,
  unless $l$ is the last layer - in that case the output of $l$ is the
  output $y_1,\dots,y_m$ of the \ac{mlp}.
\end{enumerate}

The weights of the \acp{an} are initialized to random values and, in order
to have meaningful outputs, the \ac{ann} needs to be trained. In a
supervised learning framework the dataset is composed of the matrices
$\matr{X}$ ($N\times n$) of the $N$ inputs $x_{i,j}$, and $\matr{Y}$
($N\times m$) of 
the corresponding outputs $y_{i,j}$. The
training process is called \emph{backpropagation} and it is organized in a
succession of phases. For each phase $p$ there are two steps:
\begin{description}
\item[execution] where an input $x_{p,1},\dots x_{p,n}$ is given to
  the network and an output $\hat{y}_{p,1},\dots,\hat{y}_{p,m}$ is calculated;
\item[weight update] where is calculated the error between
  $\hat{y}_{p,1},\dots,\hat{y}_{p,m}$ and the correct output
  $y_{p,1},\dots,y_{p,m}$, this error is back propagated through all
  the layers and a correction $\Delta w_i$ is calculated for each weight $w_i$ of
  the network, in
  order to minimize the error surface in the space of the weights.
\end{description}
In detail, to calculate the weights $\vect{w}^{(p+1)}$ for the next
phase $p+1$, it is 
sufficient to determine the gradient of 
the error surface in the current point $\vect{w}^{(t)}$ in order to
apply an optimization method like \ac{sgd}.

\subsection{\acf{rnn}}
\acp{rnn} are specialized versions of \ac{ann} for sequences. They
exhibit an
internal state $\vect{h}$ that changes during the training and that
recursively depends on the state of the previous phase. Precisely we
have \cref{eq:rnnState}:
\begin{equation}\label{eq:rnnState}
  \vect{h}^{(t)} = f(\vect{h}^{(t-1)}, \vect{x}^{(t)}; \vect{\theta}),
\end{equation}
where $\vect{h}$ is the state vector, $\vect{x}$ is the input vector
and $\vect{\theta}$ are the hyperparameters of the state-function
$f$. The index $t$ indicates 
the iteration number, and can be interpreted as a discrete time or
more in general as the progressive number of the sequence that is
presented as
input.

\begin{figure}
  \centering
  % \subfigure[]{
  \subfloat[\label{fig:rnn1}]{
    \begin{tikzpicture}[x=1.5cm, y=1.5cm]
      \node [] (input) at (0,0) {$\vect{x}$};

      \node [layer] (state) at (0,1) {$\vect{h}$};
      \node [delay] (delay) at (1,1) {};

      \node [] (output) at (0,2) {$\vect{y}$};
      
      \draw [arrow] (input) -- (state);
      \draw [arrow] (state) -- (output);
      \draw (state) edge[arrow, bend left=60] (delay.north);
      \draw (delay.south) edge[arrow, bend left=60] (state);
    \end{tikzpicture}
  }\hfill
  % \subfigure[]{
  \subfloat[\label{fig:rnn2}]{
    \begin{tikzpicture}[x=1.5cm, y=1.5cm]
      \foreach \l [count=\i] in {t-1,t,t+1}{
        \node [] (input-\i) at (\i*1.2,0) {$\vect{x}^{(\l)}$};
        \node [layer] (state-\i) at (\i*1.2,1) {$\vect{h}^{(\l)}$};
        \node [] (output-\i) at (\i*1.2,2) {$\vect{y}^{(\l)}$};

        \draw [arrow] (input-\i) -- (state-\i);
        \draw [arrow] (state-\i) -- (output-\i);
      }
      \node [layer, dashed] (state-l) at (0,1) {$\vect{h}^{(t-2)}$};
      \node [layer, dashed] (state-r) at (4*1.2,1) {$\vect{h}^{(t+2)}$};

      \draw [arrow] (state-l) -- (state-1);
      \draw [arrow] (state-1) -- (state-2);
      \draw [arrow] (state-2) -- (state-3);
      \draw [arrow] (state-3) -- (state-r);
    \end{tikzpicture}
  }
  \caption{\acf{rnn}, folded (a) and unfolded (b) models.}
  \label{fig:rnn}
\end{figure}
In order to express a compact visualisation of \acp{rnn} it is
possible to use the computational graph in \cref{fig:rnn1} where the
black box is a 
delay of one iteration. An extract of the unfolding of the computation
graph is 
shown in \cref{fig:rnn2}. 

\begin{figure}
  \centering
  \begin{tikzpicture}[x=1.5cm, y=1.5cm]
    \foreach \l [count=\i] in {t,t+1,missing,\tau}{
      \ifthenelse{\equal{\l}{missing}}{
        \node [vmissing] (state-\i) at (\i*1.2,1) {};
        \node [hmissing] (state-\i) at (\i*1.2,0) {};
      }{
        \node [] (input-\i) at (\i*1.2,0) {$\vect{x}^{(\l)}$};
        \node [layer] (state-\i) at (\i*1.2,1) {$\vect{h}^{(\l)}$};
        \draw [arrow] (input-\i) -- (state-\i);
        \ifthenelse{\equal{\l}{\tau}}{
          \node [] (output-\i) at (\i*1.2,2) {$\vect{y}^{(\l)}$};
          \draw [arrow] (state-\i) -- (output-\i);
        }
      }
    }

    \node [layer, dashed] (state-l) at (0,1) {$\vect{h}^{(t-1)}$};
    \node [layer, dashed] (state-r) at (5*1.2,1) {$\vect{h}^{(\tau+1)}$};

    \draw [arrow] (state-l) -- (state-1);
    \draw [arrow] (state-1) -- (state-2);
    \draw [arrow,dashed] (state-2) -- ++(0.8,0);
    \draw [arrowInverse,dashed] (state-4) -- ++(-0.8,0);
    \draw [arrow] (state-4) -- (state-r);
  \end{tikzpicture}
  \caption{\ac{rnn} with single output after a sequence of inputs.}
  \label{fig:rnnSO}
\end{figure}
The model in \cref{fig:rnn} provides an output $\vect{y}^{(t)}$ for every
input $\vect{x}^{(t)}$. An alternative is the model of
\cref{fig:rnnSO} where an output $\vect{y}^{(\tau)}$ is provided only
after a sequence $\vect{x}^{(t)},\dots,\vect{x}^{(\tau)}$ of inputs.

In order to train a \ac{rnn}, it is sufficient to apply the
backpropagation to the entire unfolded model.

\subsection{\acf{lstm}}
The major drawback of \acp{rnn} is the \emph{vanishing
  gradient} problem during the backpropagation. The gradient
for long-term associations, propagated through many stages, tends to
become zero. A possible solution for this problem is \ac{lstm} \cite{hochreiter_1997_lstm}.

\begin{figure}
  \centering
  % \subfigure[]{
  \subfloat[]{
    \begin{tikzpicture}[x=1.5cm, y=1.5cm]
      \node at (1,-0.5) {};

      \node[delay] (delay1) at (1.6,0.8) {};
      \coordinate (input1) at (2.5,0.5);
      \coordinate (input2) at (3.5,0.5);
      \node[] (input01) at (2.5,0) {$in_1$};
      \node[] (input02) at (3.5,0) {$in_m$};
      \node[] at (3,0.2) {$\cdots$};
      \node[every neuron, label={[xshift=-2mm]input}] (inputN) at (1,2) {};
      \node[every neuron, label={[xshift=2mm, align=left, font=\tiny] input\\gate}] (inputGate) at (2,2) {};
      \node[every neuron, label={[xshift=3mm, align=left, font=\tiny] forget\\gate}] (forgetGate) at (3,2) {};
      \node[every neuron, label={[xshift=3mm, align=left, font=\tiny] output\\gate}] (outputGate) at (4,2) {};
      \node[operation] (inputTimes) at (1.5, 3) {$\times$};
      \node[operation, label={[xshift=-3mm]state}] (state) at (2, 4) {$+$};
      \node[delay] (delay2) at (2.7,3.7) {};
      \node[operation] (forgetTimes) at (2.5, 3) {$\times$};
      \node[operation] (outputTimes) at (3, 5) {$\times$};
      \coordinate (output1) at (3,5.5);
      \node (output2) at (3,6) {$out_i$};
      \node at (0.7,5.3) {$cell_i$};

      \fill[black] (input1) circle (1mm);
      \fill[black] (input2) circle (1mm);
      \fill[black] (output1) circle (1mm);
      
      \foreach \n in {input1, input2}{
        \foreach \m in {inputN, inputGate, forgetGate, outputGate}{
          \draw[arrow] (\n) -- (\m);
        }
      }
      \foreach \m in {inputGate, forgetGate, outputGate}{
        \draw[arrow] (delay1) -- (\m);
      }

      \draw[arrow] (inputN) -- (inputTimes);
      \draw[arrow] (inputGate) -- (inputTimes);
      \draw[arrow] (forgetGate) -- (forgetTimes);
      \draw[arrow] (inputTimes) -- (state);
      \draw[arrow] (forgetTimes) -- (state);
      \draw [arrow] (state) ..  controls  (0.15,4) and (0.15,1) ..  (delay1);
      \draw[arrow] (state) -- (delay2);
      \draw[arrow] (delay2) -- (forgetTimes);
      \draw[arrow] (state) -- (outputTimes);
      \draw[arrow] (outputGate) -- (outputTimes);
      \draw[line] (outputTimes) -- (output1);
      \draw[line] (output1) -- (output2);
      \draw[line] (input01) -- (input1);
      \draw[line] (input02) -- (input2);

      \path[border] (0.4,0.5) -- (4.5,0.5) -- (4.5,5.5) -- (0.4,5.5) -- (0.4,0.5); 
      

    \end{tikzpicture}
    \label{fig:lstm1}
  }\hfill
  % \subfigure[]{
  \subfloat[]{
    \begin{tikzpicture}[x=0.8cm, y=0.8cm]
      \node at (1,-2.5) {};

      % \draw[step=1.0,black,thin] (0,3) grid (4,4);
      \draw[step=1.0,black,thin] (0,3) grid (2,4);
      \draw[step=1.0,black,thin] (3,3) grid (4,4);
      \coordinate (input1) at (0.5,3);
      \coordinate (input2) at (1.5,3);
      \coordinate (input3) at (3.5,3);
      \coordinate (inputM1) at (0.5,2.5);
      \coordinate (inputM2) at (1.5,2.5);
      \coordinate (inputM3) at (3.5,2.5);
      \coordinate (output1) at (0.5,4);
      \coordinate (output2) at (1.5,4);
      \coordinate (output3) at (3.5,4);

      \coordinate (outputM) at (2,6);
      \coordinate (outputTM) at (1,6);
      
      \node at (0.5, 3.5) {$cell_1$};
      \node at (1.5, 3.5) {$cell_2$};
      \node at (2.5, 3.5) {$\cdots$};
      \node at (3.5, 3.5) {$cell_n$};
      \coordinate (pass) at (4.5, 3.5);
      \node (inputT) at (1,0) {$\vect{x}$};
      \coordinate (inputTM) at (1,1);
      \node (outputT) at (1,7) {$\vect{y}$};

      \node[delay] (delay) at (4,1) {};

      \foreach \c in {input1, input2, input3, output1, output2, output3}{
        \fill[black] (\c) circle (1mm);
      }

      \foreach \c in {inputM1, inputM2, inputM3}{
        \draw [vectorLine] (delay) ..  controls
        ($0.5*(delay)+0.5*(\c)$) and ($(\c)-(0,0.5)$) ..  (\c);
        \draw [vectorLine] (inputTM) ..  controls
        ($0.5*(inputTM)+0.5*(\c)$) and ($(\c)-(0,0.5)$) ..  (\c);
      }

      \foreach \c in {1, 2, 3}{
        \draw[vectorArrow] (inputM\c) -- ($(input\c)-(0,0.1)$);
      }

      \foreach \c in {output1, output2, output3}{
        \draw [line] (\c) ..  controls  ($(\c)+(0,0.5)$) and
        ($(outputM)-(0,0.5)$) ..  (outputM);
        \draw [line] (\c) ..  controls  ($(\c)+(0,0.5)$) and
        ($(outputTM)-(0,0.5)$) ..  (outputTM);
      }

      %\draw [vectorArrow] (outputM) ..  controls  ($(outputM)+(1,2)$) and ($(delay)+(1,0)$) ..  (delay);
      \draw [vectorArrow]
      (outputM) ..
      controls  ($(outputM)+(1,2)$) and ($(pass)+(0,1)$) ..
      (pass) ..
      controls  ($(pass)-(0,1)$) and ($(delay)+(1,0)$) ..
      (delay);
      \draw[vectorLine] (inputT) -- (inputTM);
      \draw[vectorArrow] (outputTM) -- (outputT);
    \end{tikzpicture}
    \label{fig:lstm2}
  }
  \caption{\ac{lstm} model: detail of memory cell (a), and general
    scheme (b). The black box is a delay of one iteration.}
  \label{fig:lstm}
\end{figure}
The \ac{lstm} model is structured as in \cref{fig:lstm}. It is
equivalent to a
generic \ac{rnn} where the hidden state $\vect{h}$ is a
layer of \emph{memory cells}.

In detail a single
memory cell (\cref{fig:lstm1}) has four \acf{an}. One is labelled \emph{input}
and processes the cell's 
inputs into an internal state. The other three \ac{an} are labelled as
\emph{gates} and process the cell's inputs together with the previous-phase
state in order to decide:
\begin{itemize}
\item how much of the current input must be learned
  (\emph{input gate});
\item how much of the previous-phase state must be forgotten
  (\emph{forget gate});
\item how much of the state constitutes the output (\emph{output gate}).
\end{itemize}

The memory-cells layer (\cref{fig:lstm2}) is composed of a number of
cells equal to the dimension of the output. Each cell
calculates one dimension of $\vect{y}$. The input $\vect{x}$ is copied
for every cell and, together with the previous-phase
output,
constitutes the input $in_1,\dots,in_m$ of the single cells.

\section{metrics}
We used different metrics in order to evaluate the models.
All the metrics are defined between $0$ and $1$ or between $-1$ and $1$,
higher is the value, better is the assessment.

\subsection{Accuracy}
is defined
as the ratio between the correct-classified and all documents. If $\vect{y}$
is the ground truth and $\vect{\hat y}$ is the predicted
classification vectors for $n$ samples, then the accuracy is defined
as:
\begin{equation*}
  accuracy\left(\vect{y}, \vect{\hat y}\right) \equiv
  \frac{1}{n}\sum_{i=1}^{n} 1\left(\hat y_i = y_i\right),
\end{equation*}
Where
\begin{equation*}
  1(a = b)\equiv
  \begin{cases}
    1 & \text{if }a = b,\\
    0 & \text{otherwise}.
  \end{cases}
\end{equation*}

In an
unbalanced dataset, like the one of this work, it is a biased score -
a model that predicts well only the most frequent classes, and ignore
the rest, achieve a good accuracy. To resolve this we considered also
other metrics.

\subsection{Cohen's kappa} score is usually used to asses
the agreement of two annotators \cite{cohen_coefficient_1960}. It
measures the difference between the observed agreement and the
agreement that can happen by choosing randomly the class. It can then
be used to mitigate the bias caused by the unbalanced dataset. Cohen's
kappa is defined as:
\begin{equation*}
  \kappa(\vect{y}, \vect{\hat y})\equiv \frac{p_o(\vect{y}, \vect{\hat y}) -p_e(\vect{y}, \vect{\hat y})}{1-p_e(\vect{y}, \vect{\hat y})} = 1-\frac{1-p_o(\vect{y}, \vect{\hat y})}{1-p_e(\vect{y}, \vect{\hat y})},
\end{equation*}
where $p_o$ is the observed agreement that is equal to the accuracy
and $p_e$ represents the probability of agreement by chance. For $n$
samples and $k$ classes it is
defined by:
\begin{equation*}
  p_e(\vect{y}, \vect{\hat y}) = \frac{1}{n^2}\sum_{i=1}^{k} \mu_{i}(\vect{y})\cdot\nu_{i}(\vect{\hat y}),
\end{equation*}
where $\mu_{i}$ and $\nu_{i}$ are the number of samples classified as
$i$ for the first and second classifier. They are defined as:
\begin{eqnarray*}
  \mu_i(\vect{y}) &=& \sum_{j=1}^{n}1(y_j = i)\\
  \nu_i(\vect{\hat y}) &=& \sum_{j=1}^{n}1(\hat y_j = i)
\end{eqnarray*}

\subsection{\acf{map}} is a measure used in information
retrieval \cite{manning_introduction_2008}. It expresses how well the
true classification can be retrieved in 
the first results of the classifier. We define two variants of
\ac{map}, one (MAPc) to state how well all records for a specific class are
retrieved, the other (MAPs) to asses how well the correct class is retrieved
for a specific sample. The first is defined for $n$ samples and $k$ classes,
with $\vect{Y}=\vect{y}_1,\dots,\vect{y}_k$ and $\vect{\hat
  Y}=\vect{\hat y}_1,\dots,\vect{\hat y}_k$ the ground truth and
prediction $n\times k$ matrices. The formula is:
\begin{equation*}
  MAPc(\vect{Y}, \vect{\hat Y})\equiv
  \frac{1}{k}\sum_{c=1}^{k}AveP(\vect{y}_c, \vect{\hat y}_c),
\end{equation*}
where $AveP$ is the average precision for class $c$:
\begin{equation*}
  AveP(\vect{y}, \vect{\hat y}) \equiv
  \frac{1}{\sum_{i=1}^k 1(y_i)}\sum_{j=1}^k P_j(\vect{y}, \vect{\hat y})\cdot 1(y_{\sigma_{\vect{\hat y}}(j)}),
\end{equation*}
where $1(y)$ is an indicator
function that is $1$ for the elements classified positively, $0$
otherwise. $\sigma_{\vect{\hat y}}(j)$ is a function that returns the
  index in $\vect{\hat y}$ of the $j$-th element in the ordered
  version of $\vect{\hat y}$ . $P_j$ is defined as:
\begin{equation*}
P_j(\vect{y}, \vect{\hat y}) \equiv \frac{1}{j}\sum_{c=1}^j
1(y_{\sigma_{\vect{\hat y}}(c)}).
\end{equation*}

MAPs is defined like MAPc, but on the transposed classification
matrices
\begin{equation*}
  MAPs(\vect{Y}, \vect{\hat Y})\equiv
  MAPc(\vect{Y}^T, \vect{\hat Y}^T)
\end{equation*}

\subsection{Precision} is defined for $n$ samples and binary
classifications as:
\begin{equation}\label{eq:precision}
P(\vect{y}, \vect{\hat y}) \equiv \frac{\sum_{s=1}^n 1(\hat
  y_s \text{ and }y_s)}{\sum_{s=1}^n 1(\hat y_s)}.
\end{equation}
It expresses the ratio of correct positive predictions over all the
positive predictions.

\subsection{Recall} conversely, is defined as:
\begin{equation}\label{eq:recall}
R(\vect{y}, \vect{\hat y}) \equiv \frac{\sum_{s=1}^n 1(\hat
  y_s \text{ and }y_s)}{\sum_{s=1}^n 1(y_s)},
\end{equation}
and it is the ratio of correct predicted positive over all the positive
classes.

\subsection{F1-score} is the harmonic mean of precision and recall,
combining the two measures:
\begin{equation*}
F_1(\vect{y}, \vect{\hat y}) \equiv 
2\frac{P(\vect{y}, \vect{\hat y})\cdot R(\vect{y}, \vect{\hat y})}
{P(\vect{y}, \vect{\hat y})+R(\vect{y}, \vect{\hat y})}.
\end{equation*}

Precision, recall, and thus $F_1$ score are defined only for binary
classifiers. In order to use those metrics in a multi-class
classification problem it is possible to average the measures for the
different classes. We considered different methodologies of averaging:
\begin{description}
  \item[micro] averaging is performed flattening the array of truth
    and prediction of the different classes and then appling the
    scoring formula;
  \item[macro] average is performed calculating the metrics on the
    single classes and then averaging them:
    \begin{equation*}
      \frac{1}{k}\sum_{c=1}^k S(\vect{y}_c, \vect{\hat y}_c);
    \end{equation*}
  \item[weighted] average uses the normalized number of samples for
    each class in order to give a weight to them:
    \begin{equation*}
      \frac{1}{\sum_{i=1}^k|\vect{\hat y}_i|}\sum_{c=1}^k|\vect{\hat y}_c|\cdot S(\vect{y}_c, \vect{\hat y}_c).
    \end{equation*}
\end{description}

Micro average considers all the samples equally, regardless of the
representativeness of classes in the dataset. Macro average takes
into account the unbalancement and it is more sensible to few
represented classes. Precision, recall, and $F_1$ score are
equal to the accuracy when micro averaged in a multiclass
environment. 

\subsection{ROC curve} is a graph of \emph{true positive rate} versus
\emph{false positive rate} with the change of the classifier threshold. The
true positive rate is equal to the recall defined in
\cref{eq:recall}. Conversely the false positive rate is defined as:
\begin{equation}\label{eq:fpr}
FPR(\vect{y}, \vect{\hat y}) \equiv \frac{\sum_{s=1}^n 1(\hat
  y_s \text{ and not }y_s)}{\sum_{s=1}^n 1(\text{not }y_s)}.
\end{equation}

ROC curves start from $(0,0)$ and end in $(1,1)$. The area under
the curve can be used as a metric for the classifier, a perfect
classifier has an area of $1$.

The curves can be calculated only for binary classifiers. Like for the
precision, recall and $F_1$ score, it is possible to generalise them to
multiclass problems averaging micro or macro.

\subsection{Precision-recall curve} is a graph of precision
(\cref{eq:precision}) versus 
recall (\cref{eq:recall}) with the change of the classifier
threshold. The curves start from $(0,1)$ and end in $(1,0)$. A perfect
classifier has an area under the curve of $1$.

Also precision-recall curves can be calculated only for binary
classifiers. In order to generalise them to multiclass problem it is
necessary to micro or macro average.

\section{Cancer registries}
Cancer is a major concern worldwide, as it decreases the quality of
life and leads to premature mortality. In addition it is one of the
most complex and difficult-to-treat
diseases, with significant social implications, both in terms of
mortality rate and in terms of costs associated with treatment and
disability~\cite{sullivan_delivering_2011,b._stewart_world_2014,desantis_cancer_2014,siegel_cancer_2016}.
Measuring the burden of disease is one of the main concerns of public
healthcare operators. Suitable measures are necessary to describe the general state
of populationâ€™s health, to establish public health goals and to
compare the national health status and performance of health systems
across countries. Furthermore, such studies are needed to assess the
allocation of health care and health research resources across disease
categories and to evaluate the potential costs and benefits of public
health interventions~\cite{brown_burden_2001}.

Cancer registries emerged during the last few decades as a strategic
tool to quantify the impact of the disease and to provide analytic
data to healthcare operators and decision makers.  Cancer registries
use administrative and clinical data sources in order to identify all
the new cancer diagnoses in a specific area and time period and
collect incidence records that provide details on the diagnosis and
the outcome of treatments.  Mining cancer registry datasets can help
towards the development of global surveillance
programs~\cite{tourassi_deep_2017} and can provide important insights
such as survivability~\cite{delen_predicting_2005}.  Although data
analysis software would best operate on structured representations of
the reports, pathologists normally enter data items as free text in
the local country language. This requires intelligent algorithms for
medical document information extraction, retrieval, and
classification, an area that has received significant attention in the
last few years (see, e.g.,~\cite{mujtaba_clinical_2019} for a recent
account and \cite{yim_natural_2016} for the specific case of cancer).

\section{\ac{icdo}}
Pathology reports can be classified according
to codes defined in the \ac{icdo3}
system~\cite{fritz_international_2000}, a specialization of the ICD
for the cancer domain which is internationally adopted as the standard
classification for topography and
morphology~\cite{airtum_handbook_2008}.  The development of text
analysis tools specifically devoted to the automatic classification of
incidence records according to ICD-O3 codes has been addressed in a
number of previous papers. These
works, however, have either focused on reasonably large datasets but
using simple linear classifiers based on bag-of-words representations
of text~\cite{jouhet_automated_2011,kavuluru_automatic_2013}, or have
applied recent state-of-the-art deep learning
techniques~\cite{gao_hierarchical_2018,qiu_deep_2018} but using
smaller datasets and restricted to a partial set of
tumors. Additionally, the use of deep learning techniques usually
requires accurate domain-specific word vectors (embeddings of words in
a vector space) that can be derived from word co-occurrences in large
corpora of unlabeled
text~\cite{mikolov_linguistic_2013,pennington_glove:_2014,devlin2018bert}. Large
medical corpora are easily available for English (e.g. PubMed) but not
necessarily for other languages.

A topographical \ac{icdo3} code is structured as \emph{Cmm.s} where
\emph{mm} and \emph{s} represent the main site and the subsite,
respectively. For example, \emph{C50.2}
is the code for the upper-inner quadrant (\emph{2}) of breast (\emph{50}).

A morphological \ac{icdo3} code is structured as \emph{tttt/b}
where \emph{tttt} and \emph{b} represent the cell type and the tumor
behavior (benign, uncertain, in-situ, malignant primary site,
malignant metastatic site), respectively. For example, \emph{8140/3}
is the code for an adenocarcinoma (\emph{adeno 8140};
\emph{carcinoma 3}).

\section{\ac{nlp}}
\ac{nlp} is the field of designing methods that take as input or
produce as output natural language data. Natural language is
ambiguous and variable, for instance the sentence ``i saw a woman on a
hill with a binocular'' can means that I had a binocular and with
those I saw a
woman on a hill, or that a woman was on a hill using the
binocular. This ambiguity can also be the source of problems in
specific context as the medical text, where specific countermeasures
can be adopted \cite{zhao_clinical_2019,codish2005model}.

Language is
\emph{symbolic} and \emph{discrete}, the relationship between different words,
i.e. symbols, can not be inferred from the symbols
themselves. It is possible to easily compare concepts that have a
continuous representation, e.g. two different colors in an image,
while it cannot be done easily with words without using large lookup
table or advanced methods. Language is also \emph{compositional},
letters form words, and words form sentences. Language is also
\emph{sparse}, the way in which words can be combined to form meanings
is practically infinite.

\subsection{\ac{ann} in \ac{nlp}}
\ac{ml} approaches are characterized by learning to make predictions
based on past observations. \ac{ann} approaches work by learning not
only to predict, but also to correctly represent data. A common
component of \ac{ann} applied to \ac{nlp} is the embedding layer,
i.e. a mapping from discrete symbols to continuous vectors in a
relatively low dimensional space. This representation allows to
transform the isolated symbols into mathematical objects that can be
operated on.

The principal model that is used in \ac{nlp} are \ac{rnn}. They are
capable of producing a vector that summarize the entire input
sequence. They allow abandoning the \emph{markov} assumption that was
prevalent in \ac{nlp} for decades, and designing models that can
condition on entire sentences or documents. This capability leads to
impressive gains in \emph{language-modeling}, the task of predicting
the probability of the next word in a sequence.

\subsection{Features}
The mapping from textual data to real valued vector that can be used
as input for \ac{ann} models, is called \emph{features extraction}.

When the focus entity is a word outside of a context, the main source
of information is the letters. We can look at \emph{lemma},
i.e. dictionary entry of the word, e.g. words such as ``booking'',
``booked'', ``books'' to their common lemma ``book''. This mapping is
usually performed using lemma lexicons or morphological analyzers. It
is a linguistically defined process and may not work well for forms
that are not in the lexicon or for mis-pelling. a coarser process is
called \emph{stemming}. A stemmer maps words to shorter words that are
not necessarily grammatically valid. E.g. ``picture'', ``pictures'',
and ``pictured'' will be stemmed to ``pictur''. \emph{Lexical
  resources} are dictionary that are meant to be accessed by machine
rather than by humans. They tipically contain information about words,
e.g. there are lexicons that map inflected word forms to their
possible morphological analyses, telling that a certain word may be a
singular masculine noun or a past-perfect verb.

When the focus entity is text, i.e. sentences, paragraphs documents,
the features are the count and the order of the letters and words
within the text. \emph{Bag of words} is a very common feature
extraction procedure. We look at the histogram of the words within the
text. We can also compute quantities that are directly derived from
the words and the letters such as the length of the sentence. We can
also integrate statistics based on external information. When using
bag of words, it is common to use \ac{tfidf} weighting
\cite{manning_introduction_2008}. A word $w$ in a document $d$ that is
part of a large corpus $D$ of documents is represented by:
\begin{equation*}
  \frac{\#_d(w)}{\sum_{w'\in d}\#_d(w')}\cdot\log\frac{|D|}{|d\in
    D:w\in d|},
\end{equation*}
where $\#_d(w)$ is the number of times that $w$ appears in
$d$. Besides words, one may also look at consecutive pairs or triplets
of words. These are called \emph{ngrams}. A bag of ngrams
representation is much more informative than a bag of words.

When considering a word within a sentence or a document, the features
of a word are its position within the sentence and the words or
letters surrounding it. It is common to focus on the immediate context
of a word by considering a \emph{window} surrounding it (with tipical
values of 2,5, and 10 words to each side). We may also be interested
in the absolute position of a word inside a sentence, having features
such as ``the word is the 5th in the sentence'' or ``the word appear
within the first 10 of the sentence''.

When considering more than a word within a context, we can also look
at the text \emph{distance} between them or the identities of the
words that appear between them.

Sentences in natural language have structures beyond the linear order
of their words. The structure is not directly observable and are
referred to as \emph{sintax}. While it is not observable it can be
inferred from the sentence. Specialized systems exist for the
prediction of parts of speech, syntactic trees, semantic roles,
discourse relations, and other linguistic properties. These prediction
often serve as good features for classification problems.

Different features can also be combined together. Instead of combining
them manually, we can provide a set of core features to an \ac{ann}
model and rely on the training procedure to pick up important
combinations of them.
Core features can also be learned by \ac{ann}, but enough data is
needed. The distributional hypothesis of language states that the
meaning of a word can be inferred from the contexts in which it is
used. By observing co-occurrence patterns of words across a large body
of text, it is possible to infer that a word is similar to another
word. Many algorithms were derived to make use of this property. They
can be categorized into clustering-based methods, with assign similar
words to the same cluster and represent each word by its cluster
membership \cite{miller2004name}, and embedding-based methods which
represent each word as a 
vector such that similar words (with similar distribution) have
similar vectors \cite{pennington_glove:_2014,mikolov_linguistic_2013}.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "thesis"
%%% End:
