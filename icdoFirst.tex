\chapter{\ac{icdo} classification}
\label{ch:icdoFirst}

\section{metrics}
We used different metrics in order to evaluate the models.
All the metrics are defined between $0$ and $1$ or between $-1$ and $1$,
higher is the value, better is the assessment.

\subsection{Accuracy}
is defined
as the ratio between the correct-classified and all documents. If $\vect{y}$
is the ground truth and $\vect{\hat y}$ is the predicted
classification vectors for $n$ samples, then the accuracy is defined
as:
\begin{equation*}
  accuracy\left(\vect{y}, \vect{\hat y}\right) \equiv
  \frac{1}{n}\sum_{i=1}^{n} 1\left(\hat y_i = y_i\right),
\end{equation*}
Where
\begin{equation*}
  1(a = b)\equiv
  \begin{cases}
    1 & \text{if }a = b,\\
    0 & \text{otherwise}.
  \end{cases}
\end{equation*}

In an
unbalanced dataset, like the one of this work, it is a biased score -
a model that predicts well only the most frequent classes, and ignore
the rest, achieve a good accuracy. To resolve this we considered also
other metrics.

\subsection{Cohen's kappa} score is usually used to asses
the agreement of two annotators \cite{cohen_coefficient_1960}. It
measures the difference between the observed agreement and the
agreement that can happen by choosing randomly the class. It can then
be used to mitigate the bias caused by the unbalanced dataset. Cohen's
kappa is defined as:
\begin{equation*}
  \kappa(\vect{y}, \vect{\hat y})\equiv \frac{p_o(\vect{y}, \vect{\hat y}) -p_e(\vect{y}, \vect{\hat y})}{1-p_e(\vect{y}, \vect{\hat y})} = 1-\frac{1-p_o(\vect{y}, \vect{\hat y})}{1-p_e(\vect{y}, \vect{\hat y})},
\end{equation*}
where $p_o$ is the observed agreement that is equal to the accuracy
and $p_e$ represents the probability of agreement by chance. For $n$
samples and $k$ classes it is
defined by:
\begin{equation*}
  p_e(\vect{y}, \vect{\hat y}) = \frac{1}{n^2}\sum_{i=1}^{k} \mu_{i}(\vect{y})\cdot\nu_{i}(\vect{\hat y}),
\end{equation*}
where $\mu_{i}$ and $\nu_{i}$ are the number of samples classified as
$i$ for the first and second classifier. They are defined as:
\begin{eqnarray*}
  \mu_i(\vect{y}) &=& \sum_{j=1}^{n}1(y_j = i)\\
  \nu_i(\vect{\hat y}) &=& \sum_{j=1}^{n}1(\hat y_j = i)
\end{eqnarray*}

\subsection{\acf{map}} is a measure used in information
retrieval \cite{manning_introduction_2008}. It expresses how well the
true classification can be retrieved in 
the first results of the classifier. We define two variants of
\ac{map}, one (MAPc) to state how well all records for a specific class are
retrieved, the other (MAPs) to asses how well the correct class is retrieved
for a specific sample. The first is defined for $n$ samples and $k$ classes,
with $\vect{Y}=\vect{y}_1,\dots,\vect{y}_k$ and $\vect{\hat
  Y}=\vect{\hat y}_1,\dots,\vect{\hat y}_k$ the ground truth and
prediction $n\times k$ matrices. The formula is:
\begin{equation*}
  MAPc(\vect{Y}, \vect{\hat Y})\equiv
  \frac{1}{k}\sum_{c=1}^{k}AveP(\vect{y}_c, \vect{\hat y}_c),
\end{equation*}
where $AveP$ is the average precision for class $c$:
\begin{equation*}
  AveP(\vect{y}, \vect{\hat y}) \equiv
  \frac{1}{\sum_{i=1}^k 1(y_i)}\sum_{j=1}^k P_j(\vect{y}, \vect{\hat y})\cdot 1(y_{\sigma_{\vect{\hat y}}(j)}),
\end{equation*}
where $1(y)$ is an indicator
function that is $1$ for the elements classified positively, $0$
otherwise. $\sigma_{\vect{\hat y}}(j)$ is a function that returns the
  index in $\vect{\hat y}$ of the $j$-th element in the ordered
  version of $\vect{\hat y}$ . $P_j$ is defined as:
\begin{equation*}
P_j(\vect{y}, \vect{\hat y}) \equiv \frac{1}{j}\sum_{c=1}^j
1(y_{\sigma_{\vect{\hat y}}(c)}).
\end{equation*}

MAPs is defined like MAPc, but on the transposed classification
matrices
\begin{equation*}
  MAPs(\vect{Y}, \vect{\hat Y})\equiv
  MAPc(\vect{Y}^T, \vect{\hat Y}^T)
\end{equation*}

\subsection{Precision} is defined for $n$ samples and binary
classifications as:
\begin{equation}\label{eq:precision}
P(\vect{y}, \vect{\hat y}) \equiv \frac{\sum_{s=1}^n 1(\hat
  y_s \text{ and }y_s)}{\sum_{s=1}^n 1(\hat y_s)}.
\end{equation}
It expresses the ratio of correct positive predictions over all the
positive predictions.

\subsection{Recall} conversely, is defined as:
\begin{equation}\label{eq:recall}
R(\vect{y}, \vect{\hat y}) \equiv \frac{\sum_{s=1}^n 1(\hat
  y_s \text{ and }y_s)}{\sum_{s=1}^n 1(y_s)},
\end{equation}
and it is the ratio of correct predicted positive over all the positive
classes.

\subsection{F1-score} is the harmonic mean of precision and recall,
combining the two measures:
\begin{equation*}
F_1(\vect{y}, \vect{\hat y}) \equiv 
2\frac{P(\vect{y}, \vect{\hat y})\cdot R(\vect{y}, \vect{\hat y})}
{P(\vect{y}, \vect{\hat y})+R(\vect{y}, \vect{\hat y})}.
\end{equation*}

Precision, recall, and thus $F_1$ score are defined only for binary
classifiers. In order to use those metrics in a multi-class
classification problem it is possible to average the measures for the
different classes. We considered different methodologies of averaging:
\begin{description}
  \item[micro] averaging is performed flattening the array of truth
    and prediction of the different classes and then appling the
    scoring formula;
  \item[macro] average is performed calculating the metrics on the
    single classes and then averaging them:
    \begin{equation*}
      \frac{1}{k}\sum_{c=1}^k S(\vect{y}_c, \vect{\hat y}_c);
    \end{equation*}
  \item[weighted] average uses the normalized number of samples for
    each class in order to give a weight to them:
    \begin{equation*}
      \frac{1}{\sum_{i=1}^k|\vect{\hat y}_i|}\sum_{c=1}^k|\vect{\hat y}_c|\cdot S(\vect{y}_c, \vect{\hat y}_c).
    \end{equation*}
\end{description}

Micro average considers all the samples equally, regardless of the
representativeness of classes in the dataset. Macro average takes
into account the unbalancement and it is more sensible to few
represented classes. Precision, recall, and $F_1$ score are
equal to the accuracy when micro averaged in a multiclass
environment. 

\subsection{ROC curve} is a graph of \emph{true positive rate} versus
\emph{false positive rate} with the change of the classifier threshold. The
true positive rate is equal to the recall defined in
\cref{eq:recall}. Conversely the false positive rate is defined as:
\begin{equation}\label{eq:fpr}
FPR(\vect{y}, \vect{\hat y}) \equiv \frac{\sum_{s=1}^n 1(\hat
  y_s \text{ and not }y_s)}{\sum_{s=1}^n 1(\text{not }y_s)}.
\end{equation}

ROC curves start from $(0,0)$ and end in $(1,1)$. The area under
the curve can be used as a metric for the classifier, a perfect
classifier has an area of $1$.

The curves can be calculated only for binary classifiers. Like for the
precision, recall and $F_1$ score, it is possible to generalise them to
multiclass problems averaging micro or macro.

\subsection{Precision-recall curve} is a graph of precision
(\cref{eq:precision}) versus 
recall (\cref{eq:recall}) with the change of the classifier
threshold. The curves start from $(0,1)$ and end in $(1,0)$. A perfect
classifier has an area under the curve of $1$.

Also precision-recall curves can be calculated only for binary
classifiers. In order to generalise them to multiclass problem it is
necessary to micro or macro average.

\subsection{Dataset}
\label{sec:dataset}
We collected a set of $1\,592\,385$ anatomopathological exam results
from Tuscany region in the period 2004-2013. About $6\%$
of these records refer to a positive tumor
diagnosis and have topological and morphological ICD-O3 labels,
determined by tumor registry experts. Other reports are associated
with non-cancerous tissues and with unlabeled records. When multiple
pathological records for the
same patient existed for the same tumor, experts selected the most
informative report in order to assign the ICD-O3 code to that tumor
case, leaving a set of $94\,524$ labeled tumor cases.

Histological exam records consist of three free-text fields (not all
of them always filled-in), reporting tissue macroscopy, diagnosis,
and, in some cases, the patient's anamnesis. We found that field
semantics was not always used consistently and that the amount of
provided details varied significantly from extremely synthetic to very
detailed assessments of the morphology and the diagnosis. Field length
ranged from $0$ to $1\,368$ words, with first quartile, median and
third quartile respectively 34, 62, 134. For these reasons we merged
the three text fields
into a single text document, did not apply any conflation (except for
case normalization) kept punctuation.


% The data used in this work comes from the \ac{rtt}. It is composed of
% a list of
% neoplasm records, for each one is specified:
% \begin{itemize}
%     \item a series of administrative variables, e.g.\ the date of incidence, the hospital;
%     \item \ac{icdo} codes, both first and third editions, for topography
%     and morphology;
%     \item other clinical variables, e.g.\ \emph{Gleason}, \emph{Clark}, \emph{Dukes}
%     scores.
% \end{itemize}
% \ac{icdo} codes inside neoplasm records are assigned by
% \ac{rtt} personnel, thus they can be considered reliable and used as
% ground truth for the learning models.

% Furthermore, there is an histology table containing records resulting
% from microscopy exams. Each record contains three free-text
% fields, one for the macroscopy, one for the diagnosis, and one for
% other information. Besides, the histology table contains \ac{icdo} and
% \ac{icdo3} codes resulting from the application of the preexisting
% rule-based classifier. We employ those codes in order to assess the
% existing
% system, and use it as a baseline.

% The neoplasm and histology tables can be joined using a
% neoplasm identifier. Thus connecting the text
% fields with the true \ac{icdo} codes.

% The data set is composed of $309\,852$ neoplasm and $1\,592\,385$
% histology records. When joined, the resulting records are $94\,524$.

% There are neoplasm cases
% without a related histology exam because \ac{rtt} uses also other
% sources to collect tumor cases, e.g.\ \acp{hdr}, death certificates,
% etc\dots. There are also histology exams without a related neoplasm
% case because not always an histology results in a tumor case.

% \fixme{We noticed that the differences between the text fields of the
% histology records are not pronounced. Thus we considered them as
% a single field}
% \footnote{In order to build the word vectors we considered the
% fields independently.}.

\subsection{Classifiers}
We realized four multiclass classifiers for the
histological exams. Calling $X$ the distribution of texts, $Y^s$
of site, $Y^f$ of full site (site plus subsite), $Y^t$ of type, and
$Y^b$ of behavior:
\begin{description}
  \item[\site{}] estimates $P(Y^s|X)$;
  \item[\fullSite{}] estimates $P(Y^f|X)$;
  \item[\type{}] estimates $P(Y^t|X)$;
  \item[\behaviour{}] estimates $P(Y^b|X)$.
\end{description}

% The different tasks are not fully independent: obviously \fullSite{}
% classification depends on \site{} classification, but also \type{},
% \behaviour{} and \site{} can be interdependent. Many types of tumor are
% associated to a specific site, some types even indicates a
% specific behavior. For instance, \type{} $8253$
% corresponds to the definition of \emph{bronchio-alveolar carcinoma,
%   mucinous}, and it can be only malignant (\behaviour{} $3$) and located in
% bronchus and lungs (\site{} $34$). Thus, some configurations are impossible.

% Every task is a multiclass classification problem because every
% record is assigned to a single couple of \ac{icdo} codes,
% topographical and morphological.

% We considered the third \ac{icdo} version. There are
% $94\,061$ records with \ac{icdo3} codes.
The tasks have a variable number of represented classes, resumed in
\cref{fig:numClasses}.
\begin{table}
  \center
  \caption{Number of classes for different tasks.}
  \label{fig:numClasses}
  \begin{tabular}{|l|c|}
    \hline
    task & classes \\
    \hline
    \site{} & 70 \\
    \fullSite{} & 284 \\
    \type{} & 434 \\
    \behaviour{} & 5 \\
    \hline
  \end{tabular}
\end{table}
Moreover, data is not balanced. As visible in \cref{fig:classDist},
some classes are common, many are rare.
% In order to collect more than the
% $90\%$ of the 
% records it is sufficient to take the most frequent classes: $17$ over $70$
% for \site{}, $50$ over $284$ for \fullSite{},
% $44$ over $434$ for \type{}, and $2$ over $5$ for \behaviour{}
% classifications.
\begin{figure}
  \centering
  \includegraphics[width=0.45\textwidth]{img/classDist-icdo3-site.pdf}
  \includegraphics[width=0.45\textwidth]{img/classDist-icdo3-fullSite.pdf}
  \includegraphics[width=0.45\textwidth]{img/classDist-icdo3-type.pdf}
  \includegraphics[width=0.45\textwidth]{img/classDist-icdo3-behaviour.pdf}
  \caption{Documents in classes (ordered by
    frequency).}
  \label{fig:classDist}
\end{figure}

Frequency of words in text
respects the typical Zipf's distribution with few words that cover the
majority of text, and a long tail of infrequent words.
% The $85\%$ of
% the $9\,264\,143$ words in text is covered With $500$
% over a total of $35\,216$ distinct words. 


\subsection{Training word Vectors}

\subsubsection{Use cases}
The different metrics are useful to assess models in different
situations. In case you need to retrieve records of a specific
cancer case from the register, MAPc assesses how
well the task is executed.
Conversely in case of an operator-assistance software, MAPs
assess how the correct classification
for a specific histological record is retrieved on top results. 

Cohen's kappa measures the agreement of automatic
and human annotators. Thus it is an indirect measure of the
classification correctness.

Precision and recall are two measures in trade off between them. The
former is more significant when you need a list of
correctly-classified records, at cost of not retrieving all of
them. The latter is more
meaningful when you need to retrieve the major number of cases at cost
of retrieving also some false positives. A peculiarity of the
automatic annotator is that you can change the threshold to support
higher precision or recall depending on the specific retrieving
task. Recall-precision
curves can be used to assess the correct threshold.

\section{Results}
\subsection{Models}\label{sec:models}
We realized five models:
\begin{description}
\item[\svm] a \ac{svm} trained on \ac{tfidf} representation of text
  using unigrams;
\item[\svmb] a \ac{svm} trained on \ac{tfidf} representation
  using unigrams and bigrams;
\item[\lstmng] a \ac{lstm} trained on \ac{tfidf} representation using
  bigrams;
\item[\lstmc] a mixed convolutional and \ac{lstm} model trained on
  \ac{glove} representation;
\item[\lstmb] a \ac{lstm} trained on \ac{glove} representation.
\end{description}

In models \svm{} and \svmb{} we used a \ac{tfidf}
representation ignoring terms present in less
than 3 documents or in
more than 30\% of the documents. 

In \lstmng{} we used a word embedding of $30$. The
embedding layer corresponds to a one-hot representation of the input
followed by a dense layer of $30$ neurons.

\begin{figure}
  \centering
  \includegraphics[width=0.45\textwidth]{img/gloveParameter.pdf}
  \caption{Accuracy top 1,5,10 and 15 for an intrinsic test with
    varying word vector dimension.}
  \label{fig:gloveParameter}
\end{figure}
In \lstmb{} and \lstmc{} we used the word vector representation
explained in \cref{sec:word-vectors}. We trained \ac{glove} with a window dimension of $15$,
in $50$ iterations to produce representations in dimension $60$. To
decide those parameters we developed intrinsic tests collecting
quadruples like:
$$
(melanoma,\ cute,\ duttale,\ mammella)
$$
translated:
$$
(melanoma,\ skin,\ ductal,\ breast)
$$
in order to verify if $\vect{x}_{breast}$ is near
$\vect{x}_{skin}-\vect{x}_{melanoma}+\vect{x}_{ductal}$. Then we
proceeded to confront the different parameters as in
\cref{fig:gloveParameter}.

In order to speed up the computation for the models \lstmng{}, \lstmb{}
and \lstmc{} we cut the length of text to 200. Covering completely the
$87\%$ of records.

\lstmng{} is the \ac{ann} in \cref{fig:schemeLstmng},
composed of two layers of $150$ bidirectional \ac{lstm} cells,
followed by an average pooling of the sequences, followed by a
dense \ac{relu} layer, followed by a dense \emph{softmax} layer.
The number of \acp{an} of the last two layers is
equal to the number of classes for each task.
\begin{figure}
  \centering
  \begin{tikzpicture}[node distance = 0.1cm, auto]
    \begin{scope}[start chain, every node/.style={on chain}, node distance = \schemeNodeDistance]
      \nodeInput();
      \nodeEmbedding();
      \node[support] (emb) {};
      \nodeLstm();
      \node[support] (lstmA) {};
      \nodeLstm();
      \node[support] (lstmB) {};
      \nodeAvg();
      \node[support] (avg) {};
      \nodeRelu();
      \node[support] (relu) {};
      \nodeSoftmax();
      \node[dataLabel, joined] {$outDim$};
    \end{scope}
    \node[dataLabel, above=of emb] {$200\times 30$};
    \node[dataLabel, above=of lstmA] {$200\times 300$};
    \node[dataLabel, above=of lstmB] {$200\times 300$};
    \node[dataLabel, above=of avg] {$300$};
    \node[dataLabel, above=of relu] {$outDim$};
  \end{tikzpicture}
  \caption{Scheme for \lstmng{} model.}
  \label{fig:schemeLstmng}
\end{figure}

\lstmc{}, in \cref{fig:schemeLstmc} is composed of a convolutional
filter of size two, followed by a bidirectional \ac{lstm} layer of
$150$ cells, followed by an average pooling of the sequences,
followed by a dense \ac{relu}, followed by a dense \emph{softmax}
layer.
\begin{figure}
  \centering
  \begin{tikzpicture}[node distance = 0.1cm, auto]
    \begin{scope}[start chain, every node/.style={on chain}, node distance = \schemeNodeDistance]
      \nodeInput();
      \nodeGlove();
      \node[support] (glove) {};
      \nodeConv();
      \node[support] (conv) {};
      \nodeLstm();
      \node[support] (lstm) {};
      \nodeAvg();
      \node[support] (avg) {};
      \nodeRelu();
      \node[support] (relu) {};
      \nodeSoftmax();
      \node[dataLabel, joined] {$outDim$};
    \end{scope}
    \node[dataLabel, above=of glove] {$200\times 60$};
    \node[dataLabel, above=of conv] {$199\times 100$};
    \node[dataLabel, above=of lstm] {$199\times 300$};
    \node[dataLabel, above=of avg] {$300$};
    \node[dataLabel, above=of relu] {$outDim$};
  \end{tikzpicture}
  \caption{Scheme for \lstmc{} model.}
  \label{fig:schemeLstmc}
\end{figure}

\lstmb{} in \cref{fig:schemeLstmb} is composed of two bidirectional
\ac{lstm} layer of 
$150$ cells, followed by an average pooling of the sequences,
followed by a dense \ac{relu}, followed by a dense \emph{softmax}
layer. 
\begin{figure}
  \centering
  \begin{tikzpicture}[node distance = 0.1cm, auto]
    \begin{scope}[start chain, every node/.style={on chain}, node distance = \schemeNodeDistance]
      \nodeInput();
      \nodeGlove();
      \node[support] (glove) {};
      \nodeLstm();
      \node[support] (lstmA) {};
      \nodeLstm();
      \node[support] (lstmB) {};
      \nodeAvg();
      \node[support] (avg) {};
      \nodeRelu();
      \node[support] (relu) {};
      \nodeSoftmax();
      \node[dataLabel, joined] {$outDim$};
    \end{scope}
    \node[dataLabel, above=of glove] {$200\times 60$};
    \node[dataLabel, above=of lstmA] {$200\times 300$};
    \node[dataLabel, above=of lstmB] {$200\times 300$};
    \node[dataLabel, above=of avg] {$300$};
    \node[dataLabel, above=of relu] {$outDim$};
  \end{tikzpicture}
  \caption{Scheme for \lstmb{} model.}
  \label{fig:schemeLstmb}
\end{figure}

We trained \lstmng{}, \lstmc{} and \lstmb{} minimizing
the \emph{categorical crossentropy}.

\subsection{Experiments}
\label{sec:experiments}
Machine learning models need a training dataset in order to learn how
to perform predictions. A second dataset — sampled from the same
distribution — is needed to assess the performances of the
classifier. We will use leave-one-out ten-folds cross validation
to take advantage of all the available data. The whole
pathological-record dataset is split in ten equal parts called
folds. To preserve labels distribution, this is performed in a
stratified way (all the folds have the same proportions of
labels). Afterwards, each
model is trained ten times, using one fold at a time as test
dataset and the rests as training. For each metric we summarize the
average and the standard deviation among the runs. Regarding the
curves we calculated the cumulative for each 
one, i.e.\ we concatenate the prediction for all the folds and
calculate the curves on them. 

\begin{table}
  \centering
  %\scriptsize
  \footnotesize
  %\small
  \input{tabs/site.tex}
  \caption{Results for \site{} task.}
  \label{tab:resultsSite}
\end{table}

\begin{table}
  \centering
  %\scriptsize
  \footnotesize
  %\small
  \input{tabs/fullSite.tex}
  \caption{Results for \fullSite{} task.}
  \label{tab:resultsFullSite}
\end{table}

\begin{table}
  \centering
  %\scriptsize
  \footnotesize
  %\small
  \input{tabs/type.tex}
  \caption{Results for \type{} task.}
  \label{tab:resultsType}
\end{table}

\begin{table}
  \centering
  %\scriptsize
  \footnotesize
  %\small
  \input{tabs/behaviour.tex}
  \caption{Results for \behaviour{} task.}
  \label{tab:resultsBehaviour}
\end{table}

The results are resumed in \cref{tab:resultsSite}, \cref{tab:resultsFullSite},
\cref{tab:resultsType}
and \cref{tab:resultsBehaviour}, one table for each task described
in \cref{sec:dataset}, one column for each 
model described in \cref{sec:models}. Rows specify the metrics
described in \cref{sec:metrics}, with also macro and weighted average
for precision, recall and $F_1$ score. Micro averages are not
visualized because equivalent to accuracy. Values are expressed in
percentage, indicating average and standard deviation among folds.
%, furthermore for the model \lstmb{} we added another
%task: \emph{full site stacked}. Such task is to classify the \emph{full site}
%having already the \emph{site} code assigned.

\begin{figure}
  \centering
  \resizebox{0.9\textwidth}{!}{\input{img/plots/pr-curve-sede1-macro.pgf}}
  \caption{Macro-averaged recall-precision curves for \site{} task.}
  \label{fig:curvesSite}
\end{figure}

\begin{figure}
  \centering
  \resizebox{0.9\textwidth}{!}{\input{img/plots/pr-curve-sede12-macro.pgf}}
  \caption{Macro-averaged recall-precision curves for \fullSite{} task.}
  \label{fig:curvesFullSite}
\end{figure}

\begin{figure}
  \centering
  \resizebox{0.9\textwidth}{!}{\input{img/plots/pr-curve-morfo1-macro.pgf}}
  \caption{Macro-averaged recall-precision curves for \type{} task.}
  \label{fig:curvesType}
\end{figure}

\begin{figure}
  \centering
  \resizebox{0.9\textwidth}{!}{\input{img/plots/pr-curve-morfo2-macro.pgf}}
  \caption{Macro-averaged recall-precision curves for \behaviour{} task.}
  \label{fig:curvesBehaviour}
\end{figure}

Macro-averaged recall-precision curves for every method and task are
resumed in \cref{fig:curvesSite}, \cref{fig:curvesFullSite},
\cref{fig:curvesType} and
\cref{fig:curvesBehaviour}. Areas under the curves are indicated in
plot legends.

\section{Discussion}
The aim of this work was to investigate different machine learning
models for cancer cases classification based on the interpretation
of pathological-reports free text.

Considering \ac{svm} models, the improvement of using
bigrams respect monograms is not remarkable.

The use of deep learning is not necessarily beneficial to the
classification task. In fact \lstmng{}
performs worst compared to \ac{svm} models. Instead, when we take
advantage of the unlabeled data with \ac{glove}, we notice an
improvement.

\lstmb{} performs always better than the other models, except
regarding macro-averaged precision, where \ac{svm} models are better.

Since the cancer registration process is partially based on manual
revision, including also the free text interpretation in pathological
reports, a delay in data production and publication may occur. This
weakens data relevance for the purpose of assessing compliance with
regional updated recommended integrated case pathways, as well as for
public health purposes.

Improving automated methods to generate a list of putative incident
cases and to automatically estimate process indicators is an
opportunity to perform an up-to-date evaluation of cancer-care
quality.

This work demonstrated that the machine learning approaches can be
used to provide an automated support in cancer classification based on
the information contained in free-text pathology reports.

Machine learning techniques overcome the previously mentioned
delay in cancer case definition by cancer registry and allow a
powerful tool for timely indicators computation. The implementation of
this procedure guarantee an automated and validated instrument to
monitor and evaluate diagnostic and therapeutic pathways in the
health care context.


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "thesis"
%%% End:
