\chapter{\ac{icdo} classification improved}
\label{ch:icdo}

\section{Dataset}
For the experiments in this chapter, we further refined the dataset
explained in \cref{sec:dataset}. We removed duplicate
reports and reports labeled with extremely rare (1048
samples that not
appear in either training, validation, and test sets) ICD-O3 codes. We
obtained in the end 
a dataset suitable for supervised learning consisting of $85\,170$
labeled records over $203$ morphological classes and $68$
topological classes (see below).

\section{Prediction tasks}
We defined two multi-class classification tasks: (1) main tumor site
prediction ($68$ mutually exclusive classes) and (2) morphology
prediction ($203$ mutually exclusive
classes). Although the two tasks maybe somewhat correlated, we did not
attempt multi-task approaches given the small number of tasks and
large enough size of the dataset.

As in \cref{ch:icdoFirst}, the dataset is highly unbalanced.

\subsection{Plain model}
\label{sec:model}
In our setting, a dataset $\mathcal{D}=\{(\vect{x}^{(i)},y^{(i)})\}$
consists of variable length sequence vectors $\vect{x}^{(i)}$, where
$x^{(i)}_t$, for $t=1,\dots,T^{(i)}$ is the $t$-th word in the $i$-th
document, and associated target classes
$y^{(i)}\in\{1,\dots,K\}$. Unless necessary, we will drop the
superscript in the following to keep notation simple.  Sequences will
be denoted in boldface. The RNN-based sequence classifiers used in
this work compute their predictions $f(\vect{x})$ as follows:
\begin{align}
  e_t &=& E(x_t;\theta^e),\label{eq:embed}\\
  h^f_t &=& F(e_t,h^f_{t-1};\theta^f),\label{eq:maxModelRL}\\  
  h^r_t &=& R(e_t,h^r_{t+1};\theta^r),\label{eq:maxModelRR}\\
  u_t &=& G(h_t;\theta^h),\label{eq:maxModelF}\\
  \phi & =& A(\vect{h},\vect{u};\theta^a),\label{eq:aggregation}\\
  f(\vect{x}) &=& g(\phi;\theta^c).\label{eq:maxModelC}
\end{align}
$E$ is an embedding function mapping words into $p$-dimensional real
vectors (where embedding parameters $\theta_e$ can be either
pretrained and adjustable or fixed, see Section~\ref{sec:word-vectors}
below.  Functions $F$ and $R$ corresponds to (forward and reverse)
dynamics that can be described in term of several (possibly layered)
recurrent cells. In this work we focus on \ac{gru}
cells~\cite{cho2014properties} for their simplicity and because
preliminary experiments revealed no advantages over long-short term
memory cells~\cite{hochreiter1997long} on our data. Each vector $h_t$,
the concatenation of $h^f_t$ and $h^r_t$, can be interpreted as latent
representations of the information contained at position $t$ in the
document. $G$ is an additional \ac{mlp} mapping each latent vector into a
vector $u_t$ that can be seen as contextualized representation of the
word at position $t$. $A$ is an aggregation function that creates a
single $d$-dimensional representation vector for the entire sequence
and $g$ is a softmax layer. Possible choices for the aggregator
function include:
\begin{itemize}
\item $\phi=(h^f_T,h^r_1)$, which just
  extract the extreme latent representations; these may be in
  principle sufficient since they depend on the whole sequence due to
  bidirectional dynamics; note however that this approach may require
  long-term dependencies to be effectively learned.
\item
  % $\phi = \sum_t a_t(\vect{u};\theta^a) h_t$,
  $\phi = \sum_t a_t(\vect{u};\theta^a) u_t$,
  using an attention mechanism as in~\cite{yang_hierarchical_2016}; in
  this case, (scalar) attention weights are computed as
  \begin{align*}
    c_t&=&C(\vect{u};\theta^a),\\
    a_t(\vect{u};\theta^a) &=& \frac{e^{\langle c, c_t\rangle}}
    {\sum_i{e^{\langle c, c_i\rangle}}},\\
  \end{align*}
  %$$
  %a_t(\vect{u};\theta^a) = \frac{e^{\langle \theta^a, u_t\rangle}}
  %{\sum_s{e^{\langle \theta^a, u_s\rangle}}}
  %$$
  where $C$ is a single layer that map the representation $u_t$ of the
  word to a hidden representation $c_t$, then the importance of the word is
  measured as a similarity with a context vector $c$ that is learned
  with the model and can be seen as an embedded representation of an
  high level query as in memory networks \cite{sukhbaatar2015end}.
\item $\phi = \max_t u_t$ where the maximum is taken element-wise
  along the sequence; this approach can be interpreted either as a
  simplified (parameter-less) attention mechanism or a bag-layer as
  proposed in the context of multi-instance
  learning~\cite{tibo2017network}; in this case each ``feature''
  $\phi_j$ will be positive if at least one of $u_{j,1},\dots,u_{j,T}$
  is positive; the resulting classifier will find it easy to create
  decision rules predicting a document as belonging to a certain class
  if a given set of contextualized word representations are present
  and another given set of contextualized word representations are
  absent in the sequence.
\end{itemize}

The parameters $\theta^f,\theta^r,\theta^h$ and $\theta^a$ (if
present) are determined by minimizing a loss function $\loss$
(categorical cross-entropy in our case) on training data:
\begin{equation}
  \hat \theta = \mathrm{arg}\min_\theta\sum_{(\vect{x},y)\in\mathcal{D}}\loss\left(y,f(\vect{x})\right),
\end{equation}
where $\theta=\theta^f\cup\theta^r\cup\theta^h\cup\theta^a$.

% $e:\XSet\rightarrow\RSet^k$ is an embedding function that transforms
% sequence elements in tensors $\vect{x}_{i,j}=e(x_{i,j})$ of
% dimension $k$ that can be parsed by the
% model. $g:\YSet\rightarrow[0,1]^q$ is an embedding function that
% transforms sequences labels in one-hot encodings of a coherent
% dimension $q$.
\subsection{Hierarchical model}
\label{sec:modelh}
We compared the model of \cref{sec:model} with a hierarchical setting,
similarly to other works \cite{yang_hierarchical_2016}. In this
setting our dataset $\mathcal{D}=\{\vect{x}^{(i)},y^{(i)}\}$
consists of variable length sequence of sequence vectors
$\vect{x}^{(i)}$, where $x_{s,t}^{(i)}$, for $s=1,\dots,S^{(i)}$ and
$t=1,\dots,T^{(i,s)}$ is the $t$-th word of the $s$-th sentence in the
$i$-th document, and associated target classes
$y^{(i)}\in\{1,\dots,K\}$. The prediction $f(\vect{x})$ is calculated:
\begin{align}
  e_{s,t} &=& E(x_{s,t};\theta^e),\label{eq:embedH}\\
  h^f_{s,t} &=& F(e_{s,t},h^f_{s,t-1};\theta^{f}),\label{eq:maxModelRLH}\\  
  h^r_{s,t} &=& R(e_{s,t},h^r_{s,t+1};\theta^{r}),\label{eq:maxModelRRH}\\
  u_{s,t} &=& G(h_{s,t};\theta^{h}),\label{eq:maxModelFH}\\
  \phi_s & =& A(\vect{h}_s,\vect{u}_s;\theta^{a}),\label{eq:aggregationH}\\
  h'^{f}_{s} &=& F'(\phi_{s},h'^{f}_{s-1};\theta'^{f}),\label{eq:maxModelRLHS}\\  
  h'^{r}_{s} &=& R'(\phi_{s},h'^{r}_{s+1};\theta'^{r}),\label{eq:maxModelRRHS}\\
  \phi' & =& A'(\vect{s},\vect{h}';\theta'^{a}),\label{eq:aggregationHS}\\
  f(\vect{x}) &=& g(\phi';\theta^c).\label{eq:maxModelCH}
\end{align}
As in \cref{sec:model}, $E$ is an embedding function, $F$ and $R$
corresponds to forward and reverse dynamics that process word
representations,
$h_{s,t}=h_{s,t}^f\oplus h_{s,t}^r$ is the latent representation of
the information contained at position $t$ of the $s$-th sentence,
$u_{s,t}$ is the contextualized representation of the word at position
$t$ of the $s$-th sentence, and $A$ is an aggregation function that
creates a single representation for the sentence. Furthermore $F'$ and
$R'$ corresponds to 
forward and reverse dynamics that process sentence representations,
and $A'$ is the aggregation function that creates a single
representation for the entire document. $h'_s=h'^f_s\oplus h'^r_s$
can be interpreted as the
latent representation of the information contained in the sentence $s$
for the document

\subsection{Hyperparameters}
The hyperparameters of \eqref{eq:embed}~-~\eqref{eq:maxModelC}
and \eqref{eq:embedH}~-~\eqref{eq:maxModelCH} control the structure of
the model.

$\xi^e$ is associated with the embedding layer $E$ and in our case
refers to \ac{glove} 
hyperparameters~\cite{pennington_glove:_2014}, we found with an
intrinsic evaluation that the better configuration was $60$ for the vector
size, $15$ for the window size, and $50$ iterations. $\xi^f$,
$\xi^r$, $\xi'^{f}$, and $\xi'^{r}$ define the number of
\ac{gru} layers ($\xi_{(l)}$) and the number of unit per each layer
($\xi_{(d)}$) respectively for $F$, $R$, $F'$, $R'$. 
$G$ is a \ac{mlp}, $\xi^h$ controls the number of
layers and their dimension. To limit the
hyperparameters space, we decided to
make equal-dimension stacked layers in case of more than one,
regarding $F$, $R$, and $G$. $\xi^a$ and $\xi'^a$ controls the
kind of aggregating function of $A$ and $A'$ respectively, and in case
of \emph{attention} 
it controls the dimension of the attention layer. Finally $\xi^c$ controls the
data-dependent output dimension of $g$.

In our experiments we defined five different settings that we named
with acronyms. For each one of them we explored a portion of the
hyperparameters space. In \ac{max} we used the \emph{max} aggregation
function 
in the plain model of \cref{sec:modelh}. We determined the
hyperparameters evaluating the validation set in the space (with the
better configuration underlined):
\begin{align*}
  \xi_{(l)}^f=\xi_{(l)}^r&\in&[\underline{1},2],\\
  \xi_{(d)}^f=\xi_{(d)}^r&\in&[2,4,8,16,32,64,\underline{128},256,512],\\
  \xi_{(l)}^h&\in&[\underline{1},2,4],\\
  \xi_{(d)}^h&\in&[2,4,8,16,32,64,128,256,\underline{512},1024,2048],
\end{align*}
for the \emph{main site} task, and:
\begin{align*}
  \xi_{(l)}^f=\xi_{(l)}^r&\in&[\underline{1}],\\
  \xi_{(d)}^f=\xi_{(d)}^r&\in&[2,4,8,16,32,64,\underline{128},256,512],\\
  \xi_{(l)}^h&\in&[\underline{1},2,4],\\
  \xi_{(d)}^h&\in&[2,4,8,16,32,64,\underline{128},256,512,1024,2048],
\end{align*}
for the \emph{morphology} task.

In \ac{softmax} we used the \emph{attention} aggregation function in the
plain model. The hyperparameters
space was:
\begin{align*}
  \xi_{(l)}^f=\xi_{(l)}^r&\in&[\underline{1}],\\
  \xi_{(d)}^f=\xi_{(d)}^r&\in&[64,\underline{128},256],\\
  \xi_{(l)}^h&\in&[0,\underline{1}],\\
  \xi_{(d)}^h&\in&[256,\underline{512},1024],\\
  \xi_{(d)}^a&\in&[128,\underline{256},512,1024],
\end{align*}
for the site, and:
\begin{align*}
  \xi_{(l)}^f=\xi_{(l)}^r&\in&[\underline{1}],\\
  \xi_{(d)}^f=\xi_{(d)}^r&\in&[64,128,\underline{256}],\\
  \xi_{(l)}^h&\in&[0,\underline{1}],\\
  \xi_{(d)}^h&\in&[64,\underline{128},256],\\
  \xi_{(d)}^a&\in&[128,\underline{256},512,1024],
\end{align*}
for the morphology.

In \ac{maxh} we used the \emph{max} aggregation in the hierarchical
model of \cref{sec:modelh}. The hyperparameters space was:
\begin{align*}
  \xi_{(l)}^f=\xi_{(l)}^r=\xi'^f_{(l)}=\xi'^r_{(l)}&\in&[\underline{1}],\\
  \xi_{(d)}^f=\xi_{(d)}^r=\xi'^f_{(d)}=\xi'^r_{(d)}&\in&[32,\underline{64},128,256],\\
  \xi_{(l)}^h&\in&[0,1,\underline{2},4],\\
  \xi_{(d)}^h&\in&[256,512,\underline{1024},2048],\\
\end{align*}
for the site, and:
\begin{align*}
  \xi_{(l)}^f=\xi_{(l)}^r=\xi'^f_{(l)}=\xi'^r_{(l)}&\in&[\underline{1}],\\
  \xi_{(d)}^f=\xi_{(d)}^r=\xi'^f_{(d)}=\xi'^r_{(d)}&\in&[32,\underline{64},128,256],\\
  \xi_{(l)}^h&\in&[0,\underline{1},2,4],\\
  \xi_{(d)}^h&\in&[256,512,\underline{1024},2048],\\
\end{align*}
for the morphology.

In \ac{softmaxh} we used the \emph{attention} aggregation in the
hierarchical model. The hyperparameters space was:
\begin{align*}
  \xi_{(l)}^f=\xi_{(l)}^r=\xi'^f_{(l)}=\xi'^r_{(l)}&\in&[\underline{1}],\\
  \xi_{(d)}^f=\xi_{(d)}^r=\xi'^f_{(d)}=\xi'^r_{(d)}&\in&[32,64,\underline{128},256],\\
  \xi_{(l)}^h&\in&[0,\underline{1},2,4],\\
  \xi_{(d)}^h&\in&[256,512,\underline{1024},2048],\\
  \xi_{(d)}^a=\xi'^a_{(d)}&\in&[64,\underline{128},256,512],
\end{align*}
for the site, and:
\begin{align*}
  \xi_{(l)}^f=\xi_{(l)}^r=\xi'^f_{(l)}=\xi'^r_{(l)}&\in&[\underline{1}],\\
  \xi_{(d)}^f=\xi_{(d)}^r=\xi'^f_{(d)}=\xi'^r_{(d)}&\in&[32,\underline{64},128,256],\\
  \xi_{(l)}^h&\in&[0,\underline{1},2,4],\\
  \xi_{(d)}^h&\in&[256,512,\underline{1024},2048],\\
  \xi_{(d)}^a=\xi'^a_{(d)}&\in&[64,\underline{128},256,512],
\end{align*}
for the morphology.

In \ac{maxi} we used the \emph{max} aggregation in the plain
model. Also we set the model to be interpretable (see
\cref{sec:interpretability}). The hyperparameters space was:
\begin{align*}
  \xi_{(l)}^f=\xi_{(l)}^r&\in&[1,\underline{2},4],\\
  \xi_{(d)}^f=\xi_{(d)}^r&\in&[2,4,8,16,32,64,\underline{128},256,512],\\
  \xi_{(l)}^h&\in&[\underline{1},2,4],\\
  \xi_{(d)}^h&\in&[2,4,8,16,32,64,128,256,512,1024,2048],
\end{align*}
for the site, and:
\begin{align*}
  \xi_{(l)}^f=\xi_{(l)}^r&\in&[1,\underline{2},4],\\
  \xi_{(d)}^f=\xi_{(d)}^r&\in&[64,128,\underline{256},512],\\
  \xi_{(l)}^h&\in&[\underline{1}],\\
  \xi_{(d)}^h&\in&[],
\end{align*}
for the morphology. Note that in this setting the dimension of the
last layer of $G$ must be equal to the output dimension of the model
(and the softmax is applied directly after the aggregation $A$,
without any layer). Thus, $\xi_{(d)}^h$ refers only to the
layers before the last one, if they exists.




% The hyperparameters $\xi=\xi^l\cup\xi^r\cup\xi^h\cup\xi^c$ define the
% structure of the model. Regarding $\xi^l=\xi^r$ they define the type
% of \ac{rnn} that in our experiments can be \ac{lstm} or \ac{gru}, the
% number of stacked \ac{rnn} layers and the dimension of each
% layer. $\xi^h$ defines the number of layers in the first
% \ac{mlp} and their
% dimensions, moreover it defines the type of aggregating function $f$
% that in our experiments can be one of
% \begin{align}
%   f(\vect{a},\vect{b}) &= \left[\max(a_1,b_1),\dots,\max(a_l,b_l)\right];\\
%   f(\vect{a},\vect{b}) &= \left[\frac{a_1+b_1}{2},\dots,\frac{a_l+b_l}{2}\right];\\
%   f(\vect{a},\vect{b}) &= \left[a_1,\dots,a_l,b_1,\dots,b_l\right].\label{eq:aggregation}
% \end{align}
% $\xi^c$ defines the number of layers and their dimensions of the
% final classifier. The number of layers defined in $\xi^h$ can be
% equal to $0$. In such case, $\vect{h}_{i,j}$ is calculated:
% \begin{equation*}
%   \vect{h}_{i,j} = f(\vect{h}^l_{i,j},\vect{h}^r_{i,j}).
% \end{equation*}
% Also the number of layers defined in $\xi^c$ can be equal to $0$. In
% such case $\vect{\hat{y}}_i$ is calculated:
% \begin{equation*}
%   \vect{\hat y}_i = \max_j(\vect{h}_{i,j}).
% \end{equation*}


\section{Results}
\subsection{Artificial dataset}\label{sec:experiments}
Classic \ac{rnn} approaches exhibit some limits related to
memory. We designed an artificial experiment in
order to investigate how \ac{rnn} and \ac{max} address those
problems. The dataset 
$\mathcal{D}=(\vect{X}\in [0,\dots,9]^{n\times m},\vect{y}\in [0,1]^n)$ is
composed of $n$ sequences of length
$m$ of digits $x_{i,j}\in[0,\dots,9]$. If the $i$-th sequence contains
at least three consecutive digits
$x_{i,j-1},x_{i,j},x_{i,j+1}$ that concatenated represents a prime
number then the
sample is positive and labeled with $y_i=1$, otherwise is negative and
$y_i=0$.

We realized a series
$\mathcal{D}^{(m)}=(\vect{X}^{(m)},\vect{y}^{(m)})$ of balanced
datasets of increasing complexity. Each $\mathcal{D}^{(m)}$ has $100\ 000$
samples with 
sequence lengths of
$$
m\in[100,200,300,400,500,600,700,800,900,1000].$$
We trained two models on
all the datasets: a plain \ac{gru} with hidden dimension $32$ and a
our model with max aggregation with same hidden dimension $32$. 

\begin{figure}
  \centering
  \includegraphics[width=\floatwidth]{imgMax/accuracy-base.eps}
  \caption{Accuracy of plain \ac{gru} on $\mathcal{D}^{(m)}$ for different dimensions of $m$.}
  \label{fig:testAccBase}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=\floatwidth]{imgMax/accuracy-max.eps}
  \caption{Accuracy of our model with max aggregation on $\mathcal{D}^{(m)}$ for different dimensions of $m$.}
  \label{fig:testAccMax}
\end{figure}

In \cref{fig:testAccBase} and \cref{fig:testAccMax} we compare
the learning curves of the base model with the ours. We can conclude
that our model degrades less than the baseline under the assumption of
having the same number of parameters.

\subsection{Interpretability}
\label{sec:interpretability}
The model is flexible enough to gain interpretability under certain
assumptions. If we remove the last layer in \eqref{eq:maxModelC},
the dimension of last layer in \eqref{eq:maxModelF}
needs to be equal to the
output dimension ($1$ for the artificial dataset, $68$ for the site
task, and $203$ for the 
morphology).
We hypothesize that in this case, the values of
$u_t$ (or the weighted values $a_t(\vect{u};\sigma^a)u_t$ if we use the
attention as aggregator) collect information on the importance of the
area 
around $x_t$ for the purpose of classification task. This
information can be
used to interpret the model decision. To validate the
hypothesis we performed the same experiment of \cref{sec:experiments}
with an interpretable model with a comparable number of parameters.

\begin{figure}
  \centering
  \includegraphics[width=\floatwidth]{imgMax/accuracy-int.eps}
  \caption{Accuracy of interpretable model with max aggregation on $\mathcal{D}^{(m)}$ for different dimensions of $m$.}
  \label{fig:testAccInt}
\end{figure}

\begin{figure}
  \centering
  \footnotesize
  \begin{tabular}{|p{\floatwidth}|}
    \hline
    \input{attentions/test1.tex}\\
    \hline
    \input{attentions/test2.tex}\\
    \hline
    \input{attentions/test3.tex}\\
    \hline
    \input{attentions/test4.tex}\\
    \hline
    \input{attentions/test5.tex}\\
    \hline
    \input{attentions/test6.tex}\\
    \hline
  \end{tabular}
  \caption{Visualization of outputs prior to the max pooling for some
    samples. Red boxes represent the prime numbers ground
    truth, green highlighting represents the values of $u_t$ in
    \eqref{eq:maxModelF}.}
  \label{fig:testAttention}
\end{figure}

The degradation of the base \ac{gru} model in fig. \ref{fig:testAccInt} is worst
compared with the model with max aggregation in fig. \ref{fig:testAccMax},
but is still better compared with the baseline in
fig. \ref{fig:testAccBase}. Conversely the interpretable model gain
some insights on the decisions. As visible in
fig. \ref{fig:testAttention}, the model performs the classification
task focusing on 
the part of the sequences where the prime numbers are present.

\begin{figure}
  \centering
  \footnotesize
  \begin{tabular}{|p{\floatwidth}|}
    \hline
    \input{attentions/testS1.tex}\\
    \hline
    \input{attentions/testS2.tex}\\
    \hline
    \input{attentions/testS3.tex}\\
    \hline
  \end{tabular}
  \caption{Visualization of weighted features after softmax for some
    positive samples. Red boxes represent the prime numbers ground
    truth, green highlighting represents the values of
    $a_t(\vect{u};\theta^a)u_t$ in
    \eqref{eq:aggregation}.}
  \label{fig:testAttentionSoft}
\end{figure}
We compared also the difference of the max vs attention aggregation. In
\cref{fig:testAttentionSoft} we have empirical evidence that the
attention aggregation is not sufficient to guarantee the model
interpretability (for this artificial task). The softmax function do
not avoid the learning of 
distribute representations. The effects on the loss calculation of a
distribution with high variance before the aggregation, are the 
same of as a distribution with lower variance.

\begin{figure}
  \centering
  \includegraphics[width=\floatwidth]{imgMax/maxBaseDiff.eps}
  \caption{Max reached accuracy of model with max, attention, and base \ac{gru} on $\mathcal{D}^{(m)}$ for different dimensions of $m$.}
  \label{fig:testAccDiff}
\end{figure}
In \cref{fig:testAccDiff} we show the maximum reached accuracy for the
models, summarizing
\cref{fig:testAccBase,fig:testAccMax,fig:testAccInt} and adding the
model with the attention aggregator. The latter degrades faster than the
one with the max aggregation.

\subsection{Cancer register data}
Machine learning models need a training dataset in order to learn how
to perform predictions. A second test dataset — sampled from the same
distribution — is needed to assess the performances of the
classifier, and a third evaluation dataset is used to tune the
hyperparameters and to stop the training before the model overfits to
the training dataset. We decided to arrange our experiments on cancer
data in 
a setting that simulate a predictive task. We sorted the medical
records by insertion date, then we used the most recent $80\%$ of
records as test 
dataset, an equal amount of the remaining most recent records as
validation dataset, and the rest as training dataset. At the end the
training, validation, and test datasets have respectively $51\,101$,
$17\,034$, and $17\,035$ records.

We used the text in the $1.5$ millions unlabeled records, plus the
text in training datasets to train \ac{glove} word
vectors representations. We trained two different \ac{max} models, one
in an interpretable configuration that we call \ac{maxi} and one not
interpretable with 
best configuration that we call \ac{max}. We found the hyperparameters
configuration with grid search. 
\ac{maxi} have one \ac{gru} layer of dimension $256$
for \eqref{eq:maxModelRL} and \eqref{eq:maxModelRR}, 1 layer of
output dimension $68$ with aggregating function \eqref{eq:aggregation}
for \eqref{eq:maxModelF}, and zero layers for \eqref{eq:maxModelC}. 
\ac{max} have
one \ac{gru} layer with
dimension $128$ for \eqref{eq:maxModelRL} and \eqref{eq:maxModelRR},
one \ac{mlp} layer of dimension $512$ with aggregating function
\eqref{eq:aggregation} for \eqref{eq:maxModelF}, and
one layer of output dimension $68$ for \eqref{eq:maxModelC}.

We trained the models minimizing categorical cross entropy with Adam
\cite{kingma2014adam} using a learning rate of $0.001$. The
experiments were performed in \emph{PyTorch} on machines with
\emph{GeForce RTX 2080 Ti} 
GPU using batches of $32$ samples.

We compared \ac{max} and \ac{maxi} with a baseline of two stacked
bidirectional 
\ac{gru} layer of dimension $256$ and with \ac{bert} pretrained on the
same data where \ac{glove} where trained for the other models, and
fine-tuned on the training set.

\begin{table}
  \centering
  \caption{Results on cancer data}
  \label{tab:results}
  \begin{tabular}{|c|c|c|c|c|c|}
    \hline
    &Acc.&Acc. T3&Acc. T5&MAP&K\\
    \hline
    \hline
    \textbf{Site}&&&&&\\
    base&0.8977&0.9636&0.9764&0.9331&0.8848\\
    \acs{bert}&0.8980&0.9623&0.9768&0.9330&0.8852\\
    \acs{maxi}&0.8792&0.9533&0.9610&0.9190&0.8637\\
    \acs{maxh}&0.8980&0.9611&0.9773&0.9329&0.8851\\
    \acs{softmaxh}&0.8983&0.9617&0.9760&0.9329&0.8855\\
    \acs{softmax}&0.9008&0.9607&0.9749&0.9341&0.8883\\
    \acs{max}&\textbf{0.9024}&\textbf{0.9658}&\textbf{0.9797}&\textbf{0.9369}&\textbf{0.8901}\\
    \hline
    \hline
    \textbf{Morpho.}&&&&&\\
    base&0.8266&0.9402&0.9602&0.8881&0.8064\\
    \acs{bert}&0.8367&0.9267&0.9439&0.8870&0.8178\\
    \acs{maxi}&0.7317&0.9012&0.9305&0.8169&0.6964\\
    \acs{maxh}&0.8313&0.9382&0.9582&0.8894&0.8117\\
    \acs{softmaxh}&0.8318&0.9388&0.9566&0.8894&0.8119\\
    \acs{softmax}&\textbf{0.8424}&0.9445&0.9632&\textbf{0.8973}&\textbf{0.8240}\\
    \acs{max}&0.8394&\textbf{0.9452}&\textbf{0.9634}&0.8962&0.8207\\
    \hline
  \end{tabular}
\end{table}
In \cref{tab:results} we resumed the results of the models on the test
dataset. We report accuracy, \ac{map} and Cohen's kappa score (K).
\ac{map} is used in information
retrieval \cite{manning_introduction_2008}. It expresses how well the
correct class of a sample can be retrieved in 
the first results of the classifier output.
Cohen's kappa score is usually used to assess
the agreement of two annotators \cite{cohen_coefficient_1960}. It
measures the difference between the observed agreement and the
agreement that can happen choosing randomly the class. Can be used to
mitigate the bias caused by the unbalanced data set.

\begin{table}
  \centering
  \ttfamily
  \scriptsize
  \caption{Visualization of interpretable outputs.}
  \label{tab:multiAttention1}
  \begin{tabular}{|c|c|c|}
    \hline
    $y_i$&\textrm{Relevant} $\vect{h}_{i,j}$&$x_{i,j}$\textrm{, relevant} $\vect{h}_{i,j}$\\
    \hline
    \input{attentions/multiB3.tex}
    \hline
    \input{attentions/multiB2.tex}
    \hline
    %\input{attentions/multiB3.tex}
    %\hline
    \input{attentions/multiB4.tex}
    \hline
  \end{tabular}
\end{table}
The interpretable model is not powerful as \ac{max}, and in this task
is not even as the baseline, but it can be used as a
classification support and to 
gain insight in the classification process. In
\cref{tab:multiAttention1} we show three different samples where we
underline with different colors --- only for the indicated relevant
codes --- the values of $\vect{h}_{i,j}$ in \eqref{eq:maxModelF}. We
consider a code relevant if the corresponding value in the
68-dimension vector $\vect{h}_{i,j}$ is greater than $0.1$. In the
first sample, that was correctly classified, all the terms related to
prostate gland cancer are strongly underlined. Apart from
\emph{prostatico} and \emph{prostata} that are Italian terms for
\emph{prostatic} and \emph{prostate}, the main underlined terms are
\emph{PSA} (Prostate-Specific Antigene) and \emph{Gleason} score that
are two common exams in prostate cancer cases
\cite{brimo2013prostate}.

The second sample is a more difficult
document, it is roughly translated in english:\\
%\smallskip\noindent
\begin{small}
  \ttfamily
  \begin{tabular}{l}
    ISOLATED FRAGMENTS ATTRIBUTABLES\\
    TO HIGH DEGREE\\
    INTESTINAL TUBULAR ADENOMA.\\
    FRAGMENTS (NR. 2) OF PEDUNCULATED\\
    POLYPUS AT 20 CM FROM\\
    THE ANAL ORIFICE. (PERFORMED\\
    HEMATOXYLIN-EOSIN COLORING).
\end{tabular}
\end{small}\\
For this sample, the model propose the three classification codes \emph{18},
\emph{20}, and \emph{21} (with value $1$ for both \emph{18} and
\emph{20}, and little less for \emph{21}). It suggests that the terms
\emph{intestinal tubular adenoma} and \emph{pedunculated polypus} are
related to colon, \emph{polypus} can be related also to rectum, and
\emph{anal orifice} is related to rectum and anus. Note that this
record was labeled with the code for rectum, while the medical report
explicitly mention that the fragments have been extracted at 20 cm
from the anal orifice (the human rectum is long approximately 12 cm
and the anal canal 3-5 cm \cite{greene2006ajcc}).

The third sample is an even more complex record, in english is
translated:\\
\begin{small}
  \ttfamily
  \begin{tabular}{l}
    LEFT PLEURAL EFFUSION OF\\
    UNKNOWN ORIGIN AND LUNG\\
    THICKENING OF UNKNOWN ORIGIN,\\
    NODULES OF THE ABDOMINAL WALL.\\
    CANCEROUS INFILTRATION OF THE\\
    CONNECTIVE-ADIPOSE STROMA.\\
    IMMUNOHISTOCHEMICAL: CK7 +,\\
    CK20 -, TTF-1 -, PROTEIN\\
    S-100 -. 2 CM LESION,\\
    0 X 1,3 X 0,7. 1-2)\\
    SERIAL SECTIONS.
\end{tabular}
\end{small}\\
The model classifies the record mainly with codes \emph{34} and
\emph{80}, and less with \emph{56} and \emph{67}. It underlines with
the lung code the terms
\emph{plurial effusion} and \emph{lung thickening}, but is interesting
that it also underlines the immunohistochemical results. The
positive \emph{CK7}, negative \emph{CK20} pattern represents a common
diagnosis 
of lung origin for metastatic adenocarcinoma
\cite{kummar2002cytokeratin}. Also immunohistochemistry is a common
approach in the diagnosis of tumors of uncertain origin
\cite{duraiyan2012applications}. This can be the reason for the
underlying with code \emph{80} of the immunoistochemical part.
It is interesting to note also that \emph{pleuric} is
suggested to be related to ovary cancer, in fact the pleural cavity
constitutes the most frequently extra abdominal metastatic site in
ovarian carcinoma \cite{porcel2012pleural}.

\begin{figure}
  \centering
  \includegraphics[width=\floatwidth]{img/plotSintex.eps}
  \caption{.}
  \label{fig:sintex}
\end{figure}
To quantify the effectiveness of the interpretability, we designed an
experiment where the dataset is created taking for each document the
first $k$ words selected by  
ordering the results of the aggregator, $u_t$ in case of max,
$a_t(\vect{u};\theta^a)u_t$ in case of attention. In \cref{fig:sintex}
we plot the accuracy obtained training a plain \ac{gru} model on the
cleaned datasets, increasing $k$. 

\section{Conclusions}
Since the cancer registration process is partially based on manual
revision, including also the interpretation of the free text in
pathological reports, significant delays in data production and
publication may occur. This weakens data relevance for the purpose of
assessing compliance with updated regional recommended integrated case
pathways, as well as for public health purposes. Improving automated
methods to generate a list of putative incident cases and to
automatically estimate process indicators is thus an opportunity to
perform an up-to-date evaluation of cancer-care quality. In
particular, machine learning techniques like the ones presented in
this paper could overcome the delay in cancer case definition by the
cancer registry and allow a powerful tool for timely indicators
computation. The implementation of this procedure could guarantee an
automated and validated instrument to monitor and evaluate diagnostic
and therapeutic pathways.

We analyzed the available data and created different models in order
to implement an automated classification system. We obtained very
encouraging results in classifying cancer cases based on the
interpretation of free text in the data-flow of pathology
reports. This suggests that machine learning methods can be usefully
leveraged in this context.  We have also shown that unlabeled data can
be effectively used to construct useful word vectors and improve
classification accuracy. 

The use of administrative data sources that are up to date combined
with powerful machine learning techniques to automate text
classification is in the interest of the development of a standardized
surveillance system at Regional and National level. Stakeholders and
decision makers need timely and updated indicators to evaluate and
plan healthcare activities. The availability of timely indicators,
routinely and automatically produced, is technically possible. The
main novelty of this work is to show the power of machine learning
techniques applied to the classification of free text pathological
records. This was not yet been systematically implemented in other
Italian cancer registries. This provides a useful monitor tool for
cancer patients pathways, allowing to describe population’s general
health state and to establish public health goals.

The results of the interpretable \ac{max} model can be used to
assist the human classification process on simple records. It can be
used as
a form of text compression, highlighting the most important terms. On
more complex records it can be used to leverage the knowledge of the
model to gain insight on the decision process. To overcome the
limitations of the interpretable
model respect to the general \ac{max} model, in terms of
classification metrics, is possible to combine the two variants. The
general model can be used to give a more
authoritative classification on the samples while at the same time,
the interpretable model can highlight the same samples.


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "thesis"
%%% End:
