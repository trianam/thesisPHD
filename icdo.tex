\chapter{\ac{icdo} classification improved}
\label{ch:icdo}

\section{Dataset}
For the experiments in this chapter, we further refined the dataset
explained in \cref{sec:dataset}. We removed duplicate
reports and reports labeled with extremely rare (1048
samples that not
appear in either training, validation, and test sets) ICD-O3 codes. We
obtained in the end 
a dataset suitable for supervised learning consisting of $85\,170$
labeled records over $203$ morphological classes and $68$
topological classes (see below).

\section{Prediction tasks}
We defined two multi-class classification tasks: (1) main tumor site
prediction ($68$ mutually exclusive classes) and (2) morphology
prediction ($203$ mutually exclusive
classes). Although the two tasks maybe somewhat correlated, we did not
attempt multi-task approaches given the small number of tasks and
large enough size of the dataset.

As in \cref{ch:icdoFirst}, the dataset is highly unbalanced.

\subsection{Plain model}
\label{sec:model}
In our setting, a dataset $\mathcal{D}=\{(\vect{x}^{(i)},y^{(i)})\}$
consists of variable length sequence vectors $\vect{x}^{(i)}$, where
$x^{(i)}_t$, for $t=1,\dots,T^{(i)}$ is the $t$-th word in the $i$-th
document, and associated target classes
$y^{(i)}\in\{1,\dots,K\}$. Unless necessary, we will drop the
superscript in the following to keep notation simple.  Sequences will
be denoted in boldface. The RNN-based sequence classifiers used in
this work compute their predictions $f(\vect{x})$ as follows:
\begin{align}
  e_t &=& E(x_t;\theta^e),\label{eq:embed}\\
  h^f_t &=& F(e_t,h^f_{t-1};\theta^f),\label{eq:maxModelRL}\\  
  h^r_t &=& R(e_t,h^r_{t+1};\theta^r),\label{eq:maxModelRR}\\
  u_t &=& G(h_t;\theta^h),\label{eq:maxModelF}\\
  \phi & =& A(\vect{h},\vect{u};\theta^a),\label{eq:aggregation}\\
  f(\vect{x}) &=& g(\phi;\theta^c).\label{eq:maxModelC}
\end{align}
$E$ is an embedding function mapping words into $p$-dimensional real
vectors (where embedding parameters $\theta_e$ can be either
pretrained and adjustable or fixed, see Section~\ref{sec:word-vectors}
below.  Functions $F$ and $R$ corresponds to (forward and reverse)
dynamics that can be described in term of several (possibly layered)
recurrent cells. In this work we focus on \ac{gru}
cells~\cite{cho2014properties} for their simplicity and because
preliminary experiments revealed no advantages over long-short term
memory cells~\cite{hochreiter1997long} on our data. Each vector $h_t$,
the concatenation of $h^f_t$ and $h^r_t$, can be interpreted as latent
representations of the information contained at position $t$ in the
document. $G$ is an additional \ac{mlp} mapping each latent vector into a
vector $u_t$ that can be seen as contextualized representation of the
word at position $t$. $A$ is an aggregation function that creates a
single $d$-dimensional representation vector for the entire sequence
and $g$ is a softmax layer. Possible choices for the aggregator
function include:
\begin{itemize}
\item $\phi=(h^f_T,h^r_1)$, which just
  extract the extreme latent representations; these may be in
  principle sufficient since they depend on the whole sequence due to
  bidirectional dynamics; note however that this approach may require
  long-term dependencies to be effectively learned.
\item
  % $\phi = \sum_t a_t(\vect{u};\theta^a) h_t$,
  $\phi = \sum_t a_t(\vect{u};\theta^a) u_t$,
  using an attention mechanism as in~\cite{yang_hierarchical_2016}; in
  this case, (scalar) attention weights are computed as
  \begin{align*}
    c_t&=&C(\vect{u};\theta^a),\\
    a_t(\vect{u};\theta^a) &=& \frac{e^{\langle c, c_t\rangle}}
    {\sum_i{e^{\langle c, c_i\rangle}}},\\
  \end{align*}
  %$$
  %a_t(\vect{u};\theta^a) = \frac{e^{\langle \theta^a, u_t\rangle}}
  %{\sum_s{e^{\langle \theta^a, u_s\rangle}}}
  %$$
  where $C$ is a single layer that map the representation $u_t$ of the
  word to a hidden representation $c_t$, then the importance of the word is
  measured as a similarity with a context vector $c$ that is learned
  with the model and can be seen as an embedded representation of an
  high level query as in memory networks \cite{sukhbaatar2015end}.
\item $\phi = \max_t u_t$ where the maximum is taken element-wise
  along the sequence; this approach can be interpreted either as a
  simplified (parameter-less) attention mechanism or a bag-layer as
  proposed in the context of multi-instance
  learning~\cite{tibo2017network}; in this case each ``feature''
  $\phi_j$ will be positive if at least one of $u_{j,1},\dots,u_{j,T}$
  is positive; the resulting classifier will find it easy to create
  decision rules predicting a document as belonging to a certain class
  if a given set of contextualized word representations are present
  and another given set of contextualized word representations are
  absent in the sequence.
\end{itemize}

The parameters $\theta^f,\theta^r,\theta^h$ and $\theta^a$ (if
present) are determined by minimizing a loss function $\loss$
(categorical cross-entropy in our case) on training data:
\begin{equation}
  \hat \theta = \mathrm{arg}\min_\theta\sum_{(\vect{x},y)\in\mathcal{D}}\loss\left(y,f(\vect{x})\right).
\end{equation}

% $e:\XSet\rightarrow\RSet^k$ is an embedding function that transforms
% sequence elements in tensors $\vect{x}_{i,j}=e(x_{i,j})$ of
% dimension $k$ that can be parsed by the
% model. $g:\YSet\rightarrow[0,1]^q$ is an embedding function that
% transforms sequences labels in one-hot encodings of a coherent
% dimension $q$.
\subsection{Hierarchical model}
\label{sec:modelh}
We compared the model of \cref{sec:model} with a hierarchical setting,
similarly to other works \cite{yang_hierarchical_2016}. In this
setting our dataset $\mathcal{D}=\{\vect{x}^{(i)},y^{(i)}\}$
consists of variable length sequence of sequence vectors
$\vect{x}^{(i)}$, where $x_{s,t}^{(i)}$, for $s=1,\dots,S^{(i)}$ and
$t=1,\dots,T^{(i,s)}$ is the $t$-th word of the $s$-th sentence in the
$i$-th document, and associated target classes
$y^{(i)}\in\{1,\dots,K\}$. The prediction $f(\vect{x})$ is calculated:
\begin{align}
  e_{s,t} &=& E(x_{s,t};\theta^e),\label{eq:embedH}\\
  h^f_{s,t} &=& F(e_{s,t},h^f_{s,t-1};\theta^{f}),\label{eq:maxModelRLH}\\  
  h^r_{s,t} &=& R(e_{s,t},h^r_{s,t+1};\theta^{r}),\label{eq:maxModelRRH}\\
  u_{s,t} &=& G(h_{s,t};\theta^{h}),\label{eq:maxModelFH}\\
  \phi_s & =& A(\vect{h}_s,\vect{u}_s;\theta^{a}),\label{eq:aggregationH}\\
  w^f_{s} &=& F(\phi_{s},w^f_{s-1};\theta^{fs}),\label{eq:maxModelRLHS}\\  
  w^r_{s} &=& R(\phi_{s},w^r_{s+1};\theta^{rs}),\label{eq:maxModelRRHS}\\
  \psi & =& A(\vect{s},\vect{w};\theta^{a}),\label{eq:aggregationHS}\\
  f(\vect{x}) &=& g(\psi;\theta^c).\label{eq:maxModelCH}
\end{align}
As in \cref{sec:model}, $E$ is an embedding function, $F$ and $R$
corresponds to forward and reverse dynamics,
$h_{s,t}=h_{s,t}^f\oplus h_{s,t}^r$ is the latent representation of
the information contained at position $t$ of the $s$-th sentence,
$u_{s,t}$ is the contextualized representation of the word at position
$t$ of the $s$-th sentence, and $A$ is an aggregation function that
creates a single representation for the entire sequence (of words in a
sentence for \eqref{eq:aggregationH}, of sentences in a document for
\eqref{eq:aggregationHS}). Furthermore, $w_s=w_s^f\oplus w_s^r$
can be interpreted as the
latent representation of the information contained in the sentence $s$
for the document

\section{Baseline}

\subsubsection{Linear classifiers}
The classic approach is to employ bag-of-words
representations of textual documents.
%In this approach, a document is 
%described by a \textit{set} or a \textit{multiset} of words.
%Multisets allow one to take into account the number of occurrences of
%a word in the document.
Vector representations of documents are easily
derived from bags-of-words either by using indicator vectors or taking
into account the number of occurrences of each word using the
\ac{tfidf}\cite{manning_introduction_2008}. In those representations,
frequent and non-specific terms receive a lower weight.

Bag-of-words representations (including those employing bigrams or
trigrams) enable the application of very simple text classifiers such
as \ac{nb} or \ac{svm} \cite{cortes-support-1995}, but they
suffer two fundamental problems. First, the relative order of terms in
the documents is lost, making it impossible to take advantage of the
syntactic structure of the sentences. Second, distinct words have an
orthogonal representation even when they are semantically
close. As detailed in the next section, word vectors can be used to
address the second limitation and also allow us to take advantage of
unlabeled data, which can be typically be obtained in large amounts
and with little cost.

\subsubsection{\acs{bert}}
\ac{bert} \cite{devlin2018bert} is a recent model that represents the
state of the art in many \ac{nlp} related tasks
\cite{chatterjee2019semeval,hu2019introductory,lee2019biobert,tshitoyan2019unsupervised}.
It is a
bi-directional pre-training model backboned by the Transformer Encoder
\cite{vaswani2017attention}. It is an attention-based technique that
learns context-dependent word representation on large unlabeled
corpora, and then the model is fine tuned end to end on specific labeled
tasks. During pre-training, the model is trained
on unlabeled data over two different tasks. In \ac{mlm} some tokens
are masked and the model is trained to predict those token based on
the context. In \ac{nsp} the model is trained to understand the
relationship between sentences predicting if two sentences are actually
consecutive or if they where randomly replaced (with 50\%
probability). After the pre-training, the model is fine-tuned to the
specific task.


\subsection{Hyperparameters}
The hyperparameters defined in \eqref{eq:embed}~-~\eqref{eq:maxModelC}
and \eqref{eq:embedH}~-~\eqref{eq:maxModelCH} control the structure of
the model. $\theta^e$ refers to \ac{glove}
hyperparameters~\cite{pennington_glove:_2014}, we found with an
intrinsic evaluation the better configuration was $60$ for the vector
size, $15$ for the window size, and $50$ iterations. $\theta^f$,
$\theta^r$, $\theta^{fs}$, and $\theta^{rs}$ set the number of
\ac{gru} layers ($\theta_{(l)}$) and the number of unit per each layer
($\theta_{(d)}$). 
layer. $G$ is a \ac{mlp}, thus $\theta^h$ controls the number of
layers and the dimension. To limit the
hyperparameters space, we decided to
make equal-dimension stacked layers in case of more than one,
regarding $F$, $R$, and $G$. $\theta^a$ controls the
kind of aggregating function of $A$, and in case of \emph{attention}
it controls the dimension of the attention layer. Finally $\theta^c$ controls the
data-dependent output dimension.

In our experiments we defined five different settings that we named
with acronyms. In \ac{max} we used the \emph{max} aggregation function
in the plain model of \cref{sec:modelh}. We determined the
hyperparameters evaluating the validation set in the space (with the
better configuration underlined):
\begin{align*}
  \theta_{(l)}^f=\theta_{(l)}^r&\in&[\underline{1},2],\\
  \theta_{(d)}^f=\theta_{(d)}^r&\in&[2,4,8,16,32,64,\underline{128},256,512],\\
  \theta_{(l)}^h&\in&[\underline{1},2,4],\\
  \theta_{(d)}^h&\in&[2,4,8,16,32,64,128,256,\underline{512},1024,2048],
\end{align*}
for the \emph{main site} task, and:
\begin{align*}
  \theta_{(l)}^f=\theta_{(l)}^r&\in&[\underline{1}],\\
  \theta_{(d)}^f=\theta_{(d)}^r&\in&[2,4,8,16,32,64,\underline{128},256,512],\\
  \theta_{(l)}^h&\in&[\underline{1},2,4],\\
  \theta_{(d)}^h&\in&[2,4,8,16,32,64,\underline{128},256,512,1024,2048],
\end{align*}
for the \emph{morphology} task.

In \ac{softmax} we used the \emph{attention} aggregation function in the
plain model. The hyperparameters
space was:
\begin{align*}
  \theta_{(l)}^f=\theta_{(l)}^r&\in&[\underline{1}],\\
  \theta_{(d)}^f=\theta_{(d)}^r&\in&[64,\underline{128},256],\\
  \theta_{(l)}^h&\in&[0,\underline{1}],\\
  \theta_{(d)}^h&\in&[256,\underline{512},1024],\\
  \theta_{(d)}^a&\in&[128,\underline{256},512,1024],
\end{align*}
for the site, and:
\begin{align*}
  \theta_{(l)}^f=\theta_{(l)}^r&\in&[\underline{1}],\\
  \theta_{(d)}^f=\theta_{(d)}^r&\in&[64,128,\underline{256}],\\
  \theta_{(l)}^h&\in&[0,\underline{1}],\\
  \theta_{(d)}^h&\in&[64,\underline{128},256],\\
  \theta_{(d)}^a&\in&[128,\underline{256},512,1024],
\end{align*}
for the morphology.

In \ac{maxh} we used the \emph{max} aggregation in the hierarchical
model of \cref{sec:modelh}. The hyperparameters space was:
\begin{align*}
  \theta_{(l)}^f=\theta_{(l)}^r=\theta_{(l)}^{fs}=\theta_{(l)}^{rs}&\in&[\underline{1}],\\
  \theta_{(d)}^f=\theta_{(d)}^r=\theta_{(d)}^{fs}=\theta_{(d)}^{rs}&\in&[32,\underline{64},128,256],\\
  \theta_{(l)}^h&\in&[0,1,\underline{2},4],\\
  \theta_{(d)}^h&\in&[256,512,\underline{1024},2048],\\
\end{align*}
for the site, and:
\begin{align*}
  \theta_{(l)}^f=\theta_{(l)}^r=\theta_{(l)}^{fs}=\theta_{(l)}^{rs}&\in&[\underline{1}],\\
  \theta_{(d)}^f=\theta_{(d)}^r=\theta_{(d)}^{fs}=\theta_{(d)}^{rs}&\in&[32,\underline{64},128,256],\\
  \theta_{(l)}^h&\in&[0,\underline{1},2,4],\\
  \theta_{(d)}^h&\in&[256,512,\underline{1024},2048],\\
\end{align*}
for the morphology.

In \ac{softmaxh} we used the \emph{attention} aggregation in the
hierarchical model. The hyperparameters space was:
\begin{align*}
  \theta_{(l)}^f=\theta_{(l)}^r=\theta_{(l)}^{fs}=\theta_{(l)}^{rs}&\in&[\underline{1}],\\
  \theta_{(d)}^f=\theta_{(d)}^r=\theta_{(d)}^{fs}=\theta_{(d)}^{rs}&\in&[32,64,\underline{128},256],\\
  \theta_{(l)}^h&\in&[0,\underline{1},2,4],\\
  \theta_{(d)}^h&\in&[256,512,\underline{1024},2048],\\
  \theta_{(d)}^a&\in&[64,\underline{128},256,512],
\end{align*}
for the site, and:
\begin{align*}
  \theta_{(l)}^f=\theta_{(l)}^r=\theta_{(l)}^{fs}=\theta_{(l)}^{rs}&\in&[\underline{1}],\\
  \theta_{(d)}^f=\theta_{(d)}^r=\theta_{(d)}^{fs}=\theta_{(d)}^{rs}&\in&[32,\underline{64},128,256],\\
  \theta_{(l)}^h&\in&[0,\underline{1},2,4],\\
  \theta_{(d)}^h&\in&[256,512,\underline{1024},2048],\\
  \theta_{(d)}^a&\in&[64,\underline{128},256,512],
\end{align*}
for the morphology.

In \ac{maxi} we used the \emph{max} aggregation in the plain
model. Also we set the model to be interpretable (see
\cref{sec:interpretability}). The hyperparameters space was:
\begin{align*}
  \theta_{(l)}^f=\theta_{(l)}^r&\in&[1,\underline{2},4],\\
  \theta_{(d)}^f=\theta_{(d)}^r&\in&[2,4,8,16,32,64,\underline{128},256,512],\\
  \theta_{(l)}^h&\in&[\underline{1},2,4],\\
  \theta_{(d)}^h&\in&[2,4,8,16,32,64,128,256,512,1024,2048],
\end{align*}
for the site, and:
\begin{align*}
  \theta_{(l)}^f=\theta_{(l)}^r&\in&[1,\underline{2},4],\\
  \theta_{(d)}^f=\theta_{(d)}^r&\in&[64,128,\underline{256},512],\\
  \theta_{(l)}^h&\in&[\underline{1}],\\
  \theta_{(d)}^h&\in&[],
\end{align*}
for the morphology. Note that in this setting the dimension of the
last layer of $G$ must be equal to the output dimension of the model
(and the softmax is applied directly after the aggregation $A$,
without any layer). Thus, $\theta_{(d)}^h$ refers only to the
layers before the last one, if they exists.




% The hyperparameters $\xi=\xi^l\cup\xi^r\cup\xi^h\cup\xi^c$ define the
% structure of the model. Regarding $\xi^l=\xi^r$ they define the type
% of \ac{rnn} that in our experiments can be \ac{lstm} or \ac{gru}, the
% number of stacked \ac{rnn} layers and the dimension of each
% layer. $\xi^h$ defines the number of layers in the first
% \ac{mlp} and their
% dimensions, moreover it defines the type of aggregating function $f$
% that in our experiments can be one of
% \begin{align}
%   f(\vect{a},\vect{b}) &= \left[\max(a_1,b_1),\dots,\max(a_l,b_l)\right];\\
%   f(\vect{a},\vect{b}) &= \left[\frac{a_1+b_1}{2},\dots,\frac{a_l+b_l}{2}\right];\\
%   f(\vect{a},\vect{b}) &= \left[a_1,\dots,a_l,b_1,\dots,b_l\right].\label{eq:aggregation}
% \end{align}
% $\xi^c$ defines the number of layers and their dimensions of the
% final classifier. The number of layers defined in $\xi^h$ can be
% equal to $0$. In such case, $\vect{h}_{i,j}$ is calculated:
% \begin{equation*}
%   \vect{h}_{i,j} = f(\vect{h}^l_{i,j},\vect{h}^r_{i,j}).
% \end{equation*}
% Also the number of layers defined in $\xi^c$ can be equal to $0$. In
% such case $\vect{\hat{y}}_i$ is calculated:
% \begin{equation*}
%   \vect{\hat y}_i = \max_j(\vect{h}_{i,j}).
% \end{equation*}


\section{Results}
\subsection{Artificial dataset}\label{sec:experiments}
Classic \ac{rnn} approaches exhibit some limits related to
memory. We designed an artificial experiment in
order to investigate how \ac{rnn} and \ac{max} address those
problems. The dataset 
$\mathcal{D}=(\vect{X}\in [0,\dots,9]^{n\times m},\vect{y}\in [0,1]^n)$ is
composed of $n$ sequences of length
$m$ of digits $x_{i,j}\in[0,\dots,9]$. If the $i$-th sequence contains
at least three consecutive digits
$x_{i,j-1},x_{i,j},x_{i,j+1}$ that concatenated represents a prime
number then the
sample is positive and labeled with $y_i=1$, otherwise is negative and
$y_i=0$.

We realized a series
$\mathcal{D}^{(m)}=(\vect{X}^{(m)},\vect{y}^{(m)})$ of balanced
datasets of increasing complexity. Each $\mathcal{D}^{(m)}$ has $100\ 000$
samples with 
sequence lengths of $m\in
[100,200,300,400,500,600,700,800,900,1000]$. We trained two models on
all the datasets: a plain \ac{gru} with hidden dimension $32$ and a
our model with max aggregation with same hidden dimension $32$. 

\begin{figure}
  \centering
  \includegraphics[width=\floatwidth]{imgMax/accuracy-base.eps}
  \caption{Accuracy of plain \ac{gru} on $\mathcal{D}^{(m)}$ for different dimensions of $m$.}
  \label{fig:testAccBase}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=\floatwidth]{imgMax/accuracy-max.eps}
  \caption{Accuracy of our model with max aggregation on $\mathcal{D}^{(m)}$ for different dimensions of $m$.}
  \label{fig:testAccMax}
\end{figure}

In \cref{fig:testAccBase} and \cref{fig:testAccMax} we compare
the learning curves of the base model with the ours. We can conclude
that our model degrades less than the baseline under the assumption of
having the same number of parameters.

\subsection{Interpretability}
\label{sec:interpretability}
The model is flexible enough to gain interpretability under certain
assumptions. If we remove the last layer in \eqref{eq:maxModelC},
the dimension of last layer in \eqref{eq:maxModelF}
needs to be equal to the
output dimension ($1$ for the artificial dataset, $68$ for the site
task, and $203$ for the 
morphology).
We hypothesize that in this case, the values of
$u_t$ (or the weighted values $a_t(\vect{u};\sigma^a)u_t$ if we use the
attention as aggregator) collect information on the importance of the
area 
around $x_t$ for the purpose of classification task. This
information can be
used to interpret the model decision. To validate the
hypothesis we performed the same experiment of \cref{sec:experiments}
with an interpretable model with a comparable number of parameters.

\begin{figure}
  \centering
  \includegraphics[width=\floatwidth]{imgMax/accuracy-int.eps}
  \caption{Accuracy of interpretable model with max aggregation on $\mathcal{D}^{(m)}$ for different dimensions of $m$.}
  \label{fig:testAccInt}
\end{figure}

\begin{figure}
  \centering
  \footnotesize
  \begin{tabular}{|p{\floatwidth}|}
    \hline
    \input{attentions/test1.tex}\\
    \hline
    \input{attentions/test2.tex}\\
    \hline
    \input{attentions/test3.tex}\\
    \hline
    \input{attentions/test4.tex}\\
    \hline
    \input{attentions/test5.tex}\\
    \hline
    \input{attentions/test6.tex}\\
    \hline
  \end{tabular}
  \caption{Visualization of outputs prior to the max pooling for some
    samples. Red boxes represent the prime numbers ground
    truth, green highlighting represents the values of $u_t$ in
    \eqref{eq:maxModelF}.}
  \label{fig:testAttention}
\end{figure}

The degradation of the base \ac{gru} model in fig. \ref{fig:testAccInt} is worst
compared with the model with max aggregation in fig. \ref{fig:testAccMax},
but is still better compared with the baseline in
fig. \ref{fig:testAccBase}. Conversely the interpretable model gain
some insights on the decisions. As visible in
fig. \ref{fig:testAttention}, the model performs the classification
task focusing on 
the part of the sequences where the prime numbers are present.

\begin{figure}
  \centering
  \footnotesize
  \begin{tabular}{|p{\floatwidth}|}
    \hline
    \input{attentions/testS1.tex}\\
    \hline
    \input{attentions/testS2.tex}\\
    \hline
    \input{attentions/testS3.tex}\\
    \hline
  \end{tabular}
  \caption{Visualization of weighted features after softmax for some
    positive samples. Red boxes represent the prime numbers ground
    truth, green highlighting represents the values of
    $a_t(\vect{u};\theta^a)u_t$ in
    \eqref{eq:aggregation}.}
  \label{fig:testAttentionSoft}
\end{figure}
We compared also the difference of the max vs attention aggregation. In
\cref{fig:testAttentionSoft} we have empirical evidence that the
attention aggregation is not sufficient to guarantee the model
interpretability (for this artificial task). The softmax function do
not avoid the learning of 
distribute representations. The effects on the loss calculation of a
distribution with high variance before the aggregation, are the 
same of as a distribution with lower variance.

\begin{figure}
  \centering
  \includegraphics[width=\floatwidth]{imgMax/maxBaseDiff.eps}
  \caption{Max reached accuracy of model with max, attention, and base \ac{gru} on $\mathcal{D}^{(m)}$ for different dimensions of $m$.}
  \label{fig:testAccDiff}
\end{figure}
In \cref{fig:testAccDiff} we show the maximum reached accuracy for the
models, summarizing
\cref{fig:testAccBase,fig:testAccMax,fig:testAccInt} and adding the
model with the attention aggregator. The latter degrades faster than the
one with the max aggregation.

\subsection{Cancer register data}
Machine learning models need a training dataset in order to learn how
to perform predictions. A second test dataset — sampled from the same
distribution — is needed to assess the performances of the
classifier, and a third evaluation dataset is used to tune the
hyperparameters and to stop the training before the model overfits to
the training dataset. We decided to arrange our experiments on cancer
data in 
a setting that simulate a predictive task. We sorted the medical
records by insertion date, then we used the most recent $80\%$ of
records as test 
dataset, an equal amount of the remaining most recent records as
validation dataset, and the rest as training dataset. At the end the
training, validation, and test datasets have respectively $51\,101$,
$17\,034$, and $17\,035$ records.

We used the text in the $1.5$ millions unlabeled records, plus the
text in training datasets to train \ac{glove} word
vectors representations. We trained two different \ac{max} models, one
in an interpretable configuration that we call \ac{maxi} and one not
interpretable with 
best configuration that we call \ac{max}. We found the hyperparameters
configuration with grid search. 
\ac{maxi} have one \ac{gru} layer of dimension $256$
for \eqref{eq:maxModelRL} and \eqref{eq:maxModelRR}, 1 layer of
output dimension $68$ with aggregating function \eqref{eq:aggregation}
for \eqref{eq:maxModelF}, and zero layers for \eqref{eq:maxModelC}. 
\ac{max} have
one \ac{gru} layer with
dimension $128$ for \eqref{eq:maxModelRL} and \eqref{eq:maxModelRR},
one \ac{mlp} layer of dimension $512$ with aggregating function
\eqref{eq:aggregation} for \eqref{eq:maxModelF}, and
one layer of output dimension $68$ for \eqref{eq:maxModelC}.

We trained the models minimizing categorical cross entropy with Adam
\cite{kingma2014adam} using a learning rate of $0.001$. The
experiments were performed in \emph{PyTorch} on machines with
\emph{GeForce RTX 2080 Ti} 
GPU using batches of $32$ samples.

We compared \ac{max} and \ac{maxi} with a baseline of two stacked
bidirectional 
\ac{gru} layer of dimension $256$ and with \ac{bert} pretrained on the
same data where \ac{glove} where trained for the other models, and
fine-tuned on the training set.

\begin{table}
  \centering
  \caption{Results on cancer data}
  \label{tab:results}
  \begin{tabular}{|c|c|c|c|c|c|}
    \hline
    &Acc.&Acc. T3&Acc. T5&MAP&K\\
    \hline
    \hline
    \textbf{Site}&&&&&\\
    base&0.8977&0.9636&0.9764&0.9331&0.8848\\
    \acs{bert}&0.8980&0.9623&0.9768&0.9330&0.8852\\
    \acs{maxi}&0.8792&0.9533&0.9610&0.9190&0.8637\\
    \acs{maxh}&0.8980&0.9611&0.9773&0.9329&0.8851\\
    \acs{softmaxh}&0.8983&0.9617&0.9760&0.9329&0.8855\\
    \acs{softmax}&0.9008&0.9607&0.9749&0.9341&0.8883\\
    \acs{max}&\textbf{0.9024}&\textbf{0.9658}&\textbf{0.9797}&\textbf{0.9369}&\textbf{0.8901}\\
    \hline
    \hline
    \textbf{Morpho.}&&&&&\\
    base&0.8266&0.9402&0.9602&0.8881&0.8064\\
    \acs{bert}&0.8367&0.9267&0.9439&0.8870&0.8178\\
    \acs{maxi}&0.7317&0.9012&0.9305&0.8169&0.6964\\
    \acs{maxh}&0.8313&0.9382&0.9582&0.8894&0.8117\\
    \acs{softmaxh}&&&&&\\
    \acs{softmax}&\textbf{0.8424}&0.9445&0.9632&\textbf{0.8973}&\textbf{0.8240}\\
    \acs{max}&0.8394&\textbf{0.9452}&\textbf{0.9634}&0.8962&0.8207\\
    \hline
  \end{tabular}
\end{table}
In \cref{tab:results} we resumed the results of the models on the test
dataset. We report accuracy, \ac{map} and Cohen's kappa score (K).
\ac{map} is used in information
retrieval \cite{manning_introduction_2008}. It expresses how well the
correct class of a sample can be retrieved in 
the first results of the classifier output.
Cohen's kappa score is usually used to assess
the agreement of two annotators \cite{cohen_coefficient_1960}. It
measures the difference between the observed agreement and the
agreement that can happen choosing randomly the class. Can be used to
mitigate the bias caused by the unbalanced data set.

\begin{table}
  \centering
  \ttfamily
  \scriptsize
  \caption{Visualization of interpretable outputs.}
  \label{tab:multiAttention1}
  \begin{tabular}{|c|c|c|}
    \hline
    $y_i$&\textrm{Relevant} $\vect{h}_{i,j}$&$x_{i,j}$\textrm{, relevant} $\vect{h}_{i,j}$\\
    \hline
    \input{attentions/multiB3.tex}
    \hline
    \input{attentions/multiB2.tex}
    \hline
    %\input{attentions/multiB3.tex}
    %\hline
    \input{attentions/multiB4.tex}
    \hline
  \end{tabular}
\end{table}
The interpretable model is not powerful as \ac{max}, and in this task
is not even as the baseline, but it can be used as a
classification support and to 
gain insight in the classification process. In
\cref{tab:multiAttention1} we show three different samples where we
underline with different colors --- only for the indicated relevant
codes --- the values of $\vect{h}_{i,j}$ in \eqref{eq:maxModelF}. We
consider a code relevant if the corresponding value in the
68-dimension vector $\vect{h}_{i,j}$ is greater than $0.1$. In the
first sample, that was correctly classified, all the terms related to
prostate gland cancer are strongly underlined. Apart from
\emph{prostatico} and \emph{prostata} that are Italian terms for
\emph{prostatic} and \emph{prostate}, the main underlined terms are
\emph{PSA} (Prostate-Specific Antigene) and \emph{Gleason} score that
are two common exams in prostate cancer cases
\cite{brimo2013prostate}.

The second sample is a more difficult
document, it is roughly translated in english:\\
%\smallskip\noindent
\begin{small}
  \ttfamily
  \begin{tabular}{l}
    ISOLATED FRAGMENTS ATTRIBUTABLES\\
    TO HIGH DEGREE\\
    INTESTINAL TUBULAR ADENOMA.\\
    FRAGMENTS (NR. 2) OF PEDUNCULATED\\
    POLYPUS AT 20 CM FROM\\
    THE ANAL ORIFICE. (PERFORMED\\
    HEMATOXYLIN-EOSIN COLORING).
\end{tabular}
\end{small}\\
For this sample, the model propose the three classification codes \emph{18},
\emph{20}, and \emph{21} (with value $1$ for both \emph{18} and
\emph{20}, and little less for \emph{21}). It suggests that the terms
\emph{intestinal tubular adenoma} and \emph{pedunculated polypus} are
related to colon, \emph{polypus} can be related also to rectum, and
\emph{anal orifice} is related to rectum and anus. Note that this
record was labeled with the code for rectum, while the medical report
explicitly mention that the fragments have been extracted at 20 cm
from the anal orifice (the human rectum is long approximately 12 cm
and the anal canal 3-5 cm \cite{greene2006ajcc}).

The third sample is an even more complex record, in english is
translated:\\
\begin{small}
  \ttfamily
  \begin{tabular}{l}
    LEFT PLEURAL EFFUSION OF\\
    UNKNOWN ORIGIN AND LUNG\\
    THICKENING OF UNKNOWN ORIGIN,\\
    NODULES OF THE ABDOMINAL WALL.\\
    CANCEROUS INFILTRATION OF THE\\
    CONNECTIVE-ADIPOSE STROMA.\\
    IMMUNOHISTOCHEMICAL: CK7 +,\\
    CK20 -, TTF-1 -, PROTEIN\\
    S-100 -. 2 CM LESION,\\
    0 X 1,3 X 0,7. 1-2)\\
    SERIAL SECTIONS.
\end{tabular}
\end{small}\\
The model classifies the record mainly with codes \emph{34} and
\emph{80}, and less with \emph{56} and \emph{67}. It underlines with
the lung code the terms
\emph{plurial effusion} and \emph{lung thickening}, but is interesting
that it also underlines the immunohistochemical results. The
positive \emph{CK7}, negative \emph{CK20} pattern represents a common
diagnosis 
of lung origin for metastatic adenocarcinoma
\cite{kummar2002cytokeratin}. Also immunohistochemistry is a common
approach in the diagnosis of tumors of uncertain origin
\cite{duraiyan2012applications}. This can be the reason for the
underlying with code \emph{80} of the immunoistochemical part.
It is interesting to note also that \emph{pleuric} is
suggested to be related to ovary cancer, in fact the pleural cavity
constitutes the most frequently extra abdominal metastatic site in
ovarian carcinoma \cite{porcel2012pleural}.

\begin{figure}
  \centering
  \includegraphics[width=\floatwidth]{img/plotSintex.eps}
  \caption{.}
  \label{fig:sintex}
\end{figure}
To quantify the effectiveness of the interpretability, we designed an
experiment where the dataset is created taking for each document the
first $k$ words selected by  
ordering the results of the aggregator, $u_t$ in case of max,
$a_t(\vect{u};\theta^a)u_t$ in case of attention. In \cref{fig:sintex}
we plot the accuracy obtained training a plain \ac{gru} model on the
cleaned datasets, increasing $k$. 



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "thesis"
%%% End:
